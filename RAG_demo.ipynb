{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciQuery: Advanced RAG System using LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ali/miniforge3/envs/sciquery_p310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from app.rag_pipeline_manager import RAGPipelineManager\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with semantic chunking and fusion retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== LLM llama3.1:latest is loaded successfully using the provider ollama\n",
      "============================== Embedding mixedbread-ai/mxbai-embed-large-v1 is loaded successfully using the provider huggingface\n",
      "pipeline will parse document False, save vector False and run injestion False.\n",
      "data/pdf_documents\n",
      "Not parsing pdf document. Its already in doc store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Vector Store created using vector db qdrant\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAGPipelineManager\n",
    "rag_manager_semantic = RAGPipelineManager(document_parsing_method=\"simple\",node_parsing_method=\"semantic\", retrieval_type = \"fusion_retrieval\")\n",
    "# Create the RAG pipeline\n",
    "rag_manager_semantic.create_query_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. However, it is revealed that much of this performance gain is due to certain system design choices and hyperparameter optimizations rather than the embedding algorithms themselves. These modications can be transferred to traditional distributional models, yielding similar gains. The context also highlights that global advantage to any single approach over others is mostly local or insignificant.\n"
     ]
    }
   ],
   "source": [
    "steps, response = rag_manager_semantic.run_query_pipeline(\"explain word embeddings?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with semantic chunk and simple retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== LLM llama3.1:latest is loaded successfully using the provider ollama\n",
      "============================== Embedding mixedbread-ai/mxbai-embed-large-v1 is loaded successfully using the provider huggingface\n",
      "pipeline will parse document False, save vector False and run injestion False.\n",
      "data/pdf_documents\n",
      "Not parsing pdf document. Its already in doc store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Vector Store created using vector db qdrant\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAGPipelineManager\n",
    "rag_manager_semantic_simple = RAGPipelineManager(document_parsing_method=\"simple\",node_parsing_method=\"semantic\", retrieval_type = \"simple\")\n",
    "# Create the RAG pipeline\n",
    "rag_manager_semantic_simple.create_query_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, word embeddings represent words as a d-dimensional vector of real numbers in a low-dimensional space, where vectors that are close to each other are shown to be semantically related. This means that words with similar meanings or contexts will have similar vector representations.\n",
      "\n",
      "The context also explains that word embeddings project discrete words to a continuous vector space, allowing for more robust representations of words, especially for those that infrequently appear in the training data. Additionally, it mentions that word vectors learned by log-linear models can exhibit linear relations among relevant words, which can be computed by simple vector addition and subtraction.\n",
      "\n",
      "Examples of linear relations mentioned in the context include \"Paris France + Rome = Italy\" and the concept of bilingual word translation tasks where a linear transform is used to project semantically identical words from one language to another.\n"
     ]
    }
   ],
   "source": [
    "steps, response = rag_manager_semantic_simple.run_query_pipeline(\"explain word embeddings\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with sentence window chunking and small-to-big retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== LLM llama3.1:latest is loaded successfully using the provider ollama\n",
      "============================== Embedding mixedbread-ai/mxbai-embed-large-v1 is loaded successfully using the provider huggingface\n",
      "pipeline will parse document False, save vector False and run injestion False.\n",
      "data/pdf_documents\n",
      "Not parsing pdf document. Its already in doc store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Vector Store created using vector db qdrant\n",
      "Adding function to fetch window for sentence window small-to-big retrieval\n"
     ]
    }
   ],
   "source": [
    "rag_manager_sent_window = RAGPipelineManager(document_parsing_method=\"simple\",node_parsing_method=\"sentence_window\", retrieval_type=\"small_to_big\")\n",
    "\n",
    "# Create the RAG pipelines\n",
    "rag_manager_sent_window.create_query_pipeline(verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here's a concise explanation of word embeddings:\n",
      "\n",
      "Word embeddings are vector representations of words that capture their semantic and syntactic information. They enable comparison of word meanings across languages, which is useful for tasks such as bilingual lexicon induction, machine translation, and cross-lingual information retrieval. Word embeddings are created by optimizing for the same objectives, with seemingly different models often being equivalent due to variations in optimization strategies, hyperparameters, and other factors.\n",
      "\n",
      "These representations can be used in various NLP applications, including named entity recognition (NER) and chunking tasks. The context also mentions that word embeddings reflect how language users organize concepts, and their similarity can indicate near-isomorphic structures across languages. However, the paper questions whether non-isomorphism is a sign of degenerate word vector spaces.\n",
      "\n",
      "Word embeddings are often learned along with other network parameters using backpropagation and can be used in conjunction with recurrent networks to produce context-sensitive representations. They have been shown to alleviate sparsity issues in supervised learning setups and are widely used in NLP systems due to their simplicity and efficacy.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "steps, response = rag_manager_sent_window.run_query_pipeline(\"explain word embeddings\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(steps[\"llm\"].inputs[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with hierarchical chunking and small-to-big retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== LLM llama3.1:latest is loaded successfully using the provider ollama\n",
      "============================== Embedding mixedbread-ai/mxbai-embed-large-v1 is loaded successfully using the provider huggingface\n",
      "pipeline will parse document False, save vector False and run injestion False.\n",
      "data/pdf_documents\n",
      "Not parsing pdf document. Its already in doc store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Vector Store created using vector db qdrant\n"
     ]
    }
   ],
   "source": [
    "rag_manager_hr = RAGPipelineManager(document_parsing_method=\"simple\",node_parsing_method=\"hierarchical\", retrieval_type=\"small_to_big\")\n",
    "\n",
    "# Create the RAG pipeline\n",
    "rag_manager_hr.create_query_pipeline(verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=\"Word embeddings provide more robust representations for words, particularly for those that infrequently appear in the training data. They implicitly capture syntactic and semantic content, allowing relations among words to be computed as the distances among their embeddings (or word vectors). This is based on the idea that a word's meaning can be determined by the company it keeps (i.e., the words it co-occurs with), projecting discrete words into a low-dimensional and continuous vector space where co-occurred words are located close to each other.\", additional_kwargs={'tool_calls': []}), raw={'model': 'llama3.1:latest', 'created_at': '2024-09-03T17:56:53.311215Z', 'message': {'role': 'assistant', 'content': \"Word embeddings provide more robust representations for words, particularly for those that infrequently appear in the training data. They implicitly capture syntactic and semantic content, allowing relations among words to be computed as the distances among their embeddings (or word vectors). This is based on the idea that a word's meaning can be determined by the company it keeps (i.e., the words it co-occurs with), projecting discrete words into a low-dimensional and continuous vector space where co-occurred words are located close to each other.\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 10496362459, 'load_duration': 43518417, 'prompt_eval_count': 628, 'prompt_eval_duration': 6674073000, 'eval_count': 105, 'eval_duration': 3775575000}, delta=None, logprobs=None, additional_kwargs={}),\n",
       " {'input': ComponentIntermediates(inputs={'topic': 'explain word embeddings'}, outputs={'topic': 'explain word embeddings'}),\n",
       "  'retrieval_prompt_tmp': ComponentIntermediates(inputs={'topic': 'explain word embeddings'}, outputs={'prompt': 'explain word embeddings'}),\n",
       "  'retriever': ComponentIntermediates(inputs={'input': 'explain word embeddings'}, outputs={'output': [NodeWithScore(node=TextNode(id_='024adeeb-8098-4ad5-8075-5ca258438d03', embedding=None, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='74c01465-8157-4bbd-9703-2902149c24b3', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='6767f54cdcfae76f5ba6e64d00f172af96343ab0e1bbb89331bebc3b32c2326a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ae76a1a6-bcd2-48f7-a236-65500506b5a4', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='e94d81cbcefd1a968dd957a3ab999db8dd1bc13af4f6ed7b99faffd3a1e265f8'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='5d5a2931-e051-420e-ae4a-e616b386a6b1', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='49e16e34d23390f4331d78b36e39ca6c41c0ed6acc0df3594bd0b5baef4560e0')}, text='Compared to conventional discrete representations (e.g., the one-hot encoding), word embedding provides more robust representations for words, particulary for those that infrequently appear in the training data. More importantly, the embedding encodessyntactic and semantic content implicitly, so that relations among words can be simply computed as the distances among their embeddings, or word vectors.', mimetype='text/plain', start_char_idx=1697, end_char_idx=2101, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.79777694), NodeWithScore(node=TextNode(id_='ae76a1a6-bcd2-48f7-a236-65500506b5a4', embedding=None, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='74c01465-8157-4bbd-9703-2902149c24b3', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='6767f54cdcfae76f5ba6e64d00f172af96343ab0e1bbb89331bebc3b32c2326a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='bde4ed88-0041-48e8-9bec-651c0866ff36', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='9946e68e30000b8227ce002efbf2d079d82ecfbe11de088a58f042916c65b8af'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='5d5a2931-e051-420e-ae4a-e616b386a6b1', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='49e16e34d23390f4331d78b36e39ca6c41c0ed6acc0df3594bd0b5baef4560e0')}, text='Following the idea that the meaning of a word can be determined by the company it keeps (Baroni and Zamparelli, 2010), i.e., the words that it co-occurs with, word embedding projects discrete words to a low-dimensional and continuous vector space where co-occurred words are located close to each other.', mimetype='text/plain', start_char_idx=1393, end_char_idx=1696, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.78733766), NodeWithScore(node=TextNode(id_='223ad6c7-3c2d-471b-a068-f6fc0f39f1c1', embedding=None, metadata={'page_label': '1', 'file_name': 'levy2015_improving_dist_sim.pdf', 'file_path': 'data/pdf_documents/levy2015_improving_dist_sim.pdf', 'file_type': 'application/pdf', 'file_size': 388830, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f795a640-6877-46c2-9404-2dacc16adb60', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'levy2015_improving_dist_sim.pdf', 'file_path': 'data/pdf_documents/levy2015_improving_dist_sim.pdf', 'file_type': 'application/pdf', 'file_size': 388830, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='8fdb7064ae3dab09686f72852f02fb9d952751c391fb3861090b1893ef55634d'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='40e6aadc-e175-4cf7-8bf2-f469ae49b19d', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'levy2015_improving_dist_sim.pdf', 'file_path': 'data/pdf_documents/levy2015_improving_dist_sim.pdf', 'file_type': 'application/pdf', 'file_size': 388830, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='0b43146702644f47861a8703a9cf758376f92858be3340366595f33d396f1b1e'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='a78aba6b-8a32-4f2a-9d20-9327f83a5e7f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'levy2015_improving_dist_sim.pdf', 'file_path': 'data/pdf_documents/levy2015_improving_dist_sim.pdf', 'file_type': 'application/pdf', 'file_size': 388830, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='afc2ca1bf5b90e47edf0181d91ca5568c4cd2bfcedf535939699a029a66c5c0e')}, text='1 Introduction Understanding the meaning of a word is at the heart of natural language processing (NLP). While a deep, human-like, understanding remains elusive, many methods have been successful in recovering certain aspects of similarity between words. Recently, neural-network based approaches in which words are embedded into a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008).', mimetype='text/plain', start_char_idx=892, end_char_idx=1328, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7809063), NodeWithScore(node=TextNode(id_='56b1fe72-c1e9-4c1e-a376-4e12889ce8f7', embedding=None, metadata={'page_label': '1', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9f199e51-a818-4561-9f5e-0f821dab2b40', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='49ba1d9b8ddc574fa8980b2b535d5efbf0d193947c55bb266f8c8a61c97d5c01'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='829a7259-b45e-4316-b830-ef097cc803b4', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='b6ef28b649fc14545dc1d2b36dd54afbcbbb5a52c83012114d896d4021268765'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='911d4937-006a-4330-a6d2-19bcf9d031d6', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='0e0d2d99ed8b89e22fc3d2ac006a4c8c001f4f4f301380131bb7ac7a67b259a6')}, text='1. Introduction In recent years, (monolingual) vector representations of words, so-called word embeddings (Mikolov, Chen, Corrado, & Dean, 2013a; Pennington, Socher, & Manning, 2014) have proven extremely useful across a wide range of natural language processing (NLP) applications.', mimetype='text/plain', start_char_idx=1202, end_char_idx=1484, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.77177525), NodeWithScore(node=TextNode(id_='af3e899a-8c3c-4e5c-8001-f9207e48afd6', embedding=None, metadata={'page_label': '3', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ad988128-1679-4384-bdf0-2d7a107fd974', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='aeac9191516f61ddcbd16d1fa2b68c5dd60402881ac634e5e5d874402f20b806'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c9418079-1b59-465f-b537-fcab1a7805f3', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '3', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='c1061a5e4095f62ca03c9f69f5e2ab7ee7b0d58f423c2c43219b749b28a37bd4'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='9c2a2090-15de-422f-982d-dc3b38c2ba0d', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '3', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='542bf158138c508fdf0c464deab0589d1ce8eba639232235c626fcaba983d5ad')}, text='LetXR|V|dbe a word embedding matrix that is learned for the -th ofLlanguages whereVis the corresponding vocabulary and dis the dimensionality of the word embeddings. We will furthermore refer to the word embedding of the i-th word in language with the shorthand x iorxiif languageis clear from context. We will refer to the word corresponding to thei-th word embedding xiaswiwherewiis a string.', mimetype='text/plain', start_char_idx=1107, end_char_idx=1501, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.77100796)]}),\n",
       "  'concat_component': ComponentIntermediates(inputs={'retrieved_nodes': [NodeWithScore(node=TextNode(id_='024adeeb-8098-4ad5-8075-5ca258438d03', embedding=None, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='74c01465-8157-4bbd-9703-2902149c24b3', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='6767f54cdcfae76f5ba6e64d00f172af96343ab0e1bbb89331bebc3b32c2326a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ae76a1a6-bcd2-48f7-a236-65500506b5a4', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='e94d81cbcefd1a968dd957a3ab999db8dd1bc13af4f6ed7b99faffd3a1e265f8'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='5d5a2931-e051-420e-ae4a-e616b386a6b1', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='49e16e34d23390f4331d78b36e39ca6c41c0ed6acc0df3594bd0b5baef4560e0')}, text='Compared to conventional discrete representations (e.g., the one-hot encoding), word embedding provides more robust representations for words, particulary for those that infrequently appear in the training data. More importantly, the embedding encodessyntactic and semantic content implicitly, so that relations among words can be simply computed as the distances among their embeddings, or word vectors.', mimetype='text/plain', start_char_idx=1697, end_char_idx=2101, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.79777694), NodeWithScore(node=TextNode(id_='ae76a1a6-bcd2-48f7-a236-65500506b5a4', embedding=None, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='74c01465-8157-4bbd-9703-2902149c24b3', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='6767f54cdcfae76f5ba6e64d00f172af96343ab0e1bbb89331bebc3b32c2326a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='bde4ed88-0041-48e8-9bec-651c0866ff36', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='9946e68e30000b8227ce002efbf2d079d82ecfbe11de088a58f042916c65b8af'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='5d5a2931-e051-420e-ae4a-e616b386a6b1', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1006', 'file_name': 'xing2015_normalized_we_orthogonal_transform.pdf', 'file_path': 'data/pdf_documents/xing2015_normalized_we_orthogonal_transform.pdf', 'file_type': 'application/pdf', 'file_size': 1157660, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='49e16e34d23390f4331d78b36e39ca6c41c0ed6acc0df3594bd0b5baef4560e0')}, text='Following the idea that the meaning of a word can be determined by the company it keeps (Baroni and Zamparelli, 2010), i.e., the words that it co-occurs with, word embedding projects discrete words to a low-dimensional and continuous vector space where co-occurred words are located close to each other.', mimetype='text/plain', start_char_idx=1393, end_char_idx=1696, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.78733766), NodeWithScore(node=TextNode(id_='223ad6c7-3c2d-471b-a068-f6fc0f39f1c1', embedding=None, metadata={'page_label': '1', 'file_name': 'levy2015_improving_dist_sim.pdf', 'file_path': 'data/pdf_documents/levy2015_improving_dist_sim.pdf', 'file_type': 'application/pdf', 'file_size': 388830, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f795a640-6877-46c2-9404-2dacc16adb60', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'levy2015_improving_dist_sim.pdf', 'file_path': 'data/pdf_documents/levy2015_improving_dist_sim.pdf', 'file_type': 'application/pdf', 'file_size': 388830, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='8fdb7064ae3dab09686f72852f02fb9d952751c391fb3861090b1893ef55634d'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='40e6aadc-e175-4cf7-8bf2-f469ae49b19d', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'levy2015_improving_dist_sim.pdf', 'file_path': 'data/pdf_documents/levy2015_improving_dist_sim.pdf', 'file_type': 'application/pdf', 'file_size': 388830, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='0b43146702644f47861a8703a9cf758376f92858be3340366595f33d396f1b1e'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='a78aba6b-8a32-4f2a-9d20-9327f83a5e7f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'levy2015_improving_dist_sim.pdf', 'file_path': 'data/pdf_documents/levy2015_improving_dist_sim.pdf', 'file_type': 'application/pdf', 'file_size': 388830, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='afc2ca1bf5b90e47edf0181d91ca5568c4cd2bfcedf535939699a029a66c5c0e')}, text='1 Introduction Understanding the meaning of a word is at the heart of natural language processing (NLP). While a deep, human-like, understanding remains elusive, many methods have been successful in recovering certain aspects of similarity between words. Recently, neural-network based approaches in which words are embedded into a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008).', mimetype='text/plain', start_char_idx=892, end_char_idx=1328, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7809063), NodeWithScore(node=TextNode(id_='56b1fe72-c1e9-4c1e-a376-4e12889ce8f7', embedding=None, metadata={'page_label': '1', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9f199e51-a818-4561-9f5e-0f821dab2b40', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='49ba1d9b8ddc574fa8980b2b535d5efbf0d193947c55bb266f8c8a61c97d5c01'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='829a7259-b45e-4316-b830-ef097cc803b4', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='b6ef28b649fc14545dc1d2b36dd54afbcbbb5a52c83012114d896d4021268765'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='911d4937-006a-4330-a6d2-19bcf9d031d6', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='0e0d2d99ed8b89e22fc3d2ac006a4c8c001f4f4f301380131bb7ac7a67b259a6')}, text='1. Introduction In recent years, (monolingual) vector representations of words, so-called word embeddings (Mikolov, Chen, Corrado, & Dean, 2013a; Pennington, Socher, & Manning, 2014) have proven extremely useful across a wide range of natural language processing (NLP) applications.', mimetype='text/plain', start_char_idx=1202, end_char_idx=1484, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.77177525), NodeWithScore(node=TextNode(id_='af3e899a-8c3c-4e5c-8001-f9207e48afd6', embedding=None, metadata={'page_label': '3', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ad988128-1679-4384-bdf0-2d7a107fd974', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='aeac9191516f61ddcbd16d1fa2b68c5dd60402881ac634e5e5d874402f20b806'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c9418079-1b59-465f-b537-fcab1a7805f3', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '3', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='c1061a5e4095f62ca03c9f69f5e2ab7ee7b0d58f423c2c43219b749b28a37bd4'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='9c2a2090-15de-422f-982d-dc3b38c2ba0d', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '3', 'file_name': 'ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_path': 'data/pdf_documents/ruder2019_survey_of_cross_lingual_we_odels.pdf', 'file_type': 'application/pdf', 'file_size': 1429688, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='542bf158138c508fdf0c464deab0589d1ce8eba639232235c626fcaba983d5ad')}, text='LetXR|V|dbe a word embedding matrix that is learned for the -th ofLlanguages whereVis the corresponding vocabulary and dis the dimensionality of the word embeddings. We will furthermore refer to the word embedding of the i-th word in language with the shorthand x iorxiif languageis clear from context. We will refer to the word corresponding to thei-th word embedding xiaswiwherewiis a string.', mimetype='text/plain', start_char_idx=1107, end_char_idx=1501, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.77100796)]}, outputs={'output': 'Compared to conventional discrete representations (e.g., the one-hot encoding), word embedding provides more robust representations for words, particulary for those that infrequently appear in the training data. More importantly, the embedding encodessyntactic and semantic content implicitly, so that relations among words can be simply computed as the distances among their embeddings, or word vectors.\\n\\nFollowing the idea that the meaning of a word can be determined by the company it keeps (Baroni and Zamparelli, 2010), i.e., the words that it co-occurs with, word embedding projects discrete words to a low-dimensional and continuous vector space where co-occurred words are located close to each other.\\n\\n1 Introduction Understanding the meaning of a word is at the heart of natural language processing (NLP). While a deep, human-like, understanding remains elusive, many methods have been successful in recovering certain aspects of similarity between words. Recently, neural-network based approaches in which words are embedded into a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008).\\n\\n1. Introduction In recent years, (monolingual) vector representations of words, so-called word embeddings (Mikolov, Chen, Corrado, & Dean, 2013a; Pennington, Socher, & Manning, 2014) have proven extremely useful across a wide range of natural language processing (NLP) applications.\\n\\nLetXR|V|dbe a word embedding matrix that is learned for the -th ofLlanguages whereVis the corresponding vocabulary and dis the dimensionality of the word embeddings. We will furthermore refer to the word embedding of the i-th word in language with the shorthand x iorxiif languageis clear from context. We will refer to the word corresponding to thei-th word embedding xiaswiwherewiis a string.'}),\n",
       "  'rag_prompt_template': ComponentIntermediates(inputs={'input': 'explain word embeddings', 'context_str': 'Compared to conventional discrete representations (e.g., the one-hot encoding), word embedding provides more robust representations for words, particulary for those that infrequently appear in the training data. More importantly, the embedding encodessyntactic and semantic content implicitly, so that relations among words can be simply computed as the distances among their embeddings, or word vectors.\\n\\nFollowing the idea that the meaning of a word can be determined by the company it keeps (Baroni and Zamparelli, 2010), i.e., the words that it co-occurs with, word embedding projects discrete words to a low-dimensional and continuous vector space where co-occurred words are located close to each other.\\n\\n1 Introduction Understanding the meaning of a word is at the heart of natural language processing (NLP). While a deep, human-like, understanding remains elusive, many methods have been successful in recovering certain aspects of similarity between words. Recently, neural-network based approaches in which words are embedded into a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008).\\n\\n1. Introduction In recent years, (monolingual) vector representations of words, so-called word embeddings (Mikolov, Chen, Corrado, & Dean, 2013a; Pennington, Socher, & Manning, 2014) have proven extremely useful across a wide range of natural language processing (NLP) applications.\\n\\nLetXR|V|dbe a word embedding matrix that is learned for the -th ofLlanguages whereVis the corresponding vocabulary and dis the dimensionality of the word embeddings. We will furthermore refer to the word embedding of the i-th word in language with the shorthand x iorxiif languageis clear from context. We will refer to the word corresponding to thei-th word embedding xiaswiwherewiis a string.'}, outputs={'prompt': \"<|begin_of_text|>\\n    <|start_header_id|>\\n        system\\n    <|end_header_id|>\\n        You are an AI assistant tasked with answering questions based on provided context.\\n        Your goal is to provide accurate, relevant, and concise responses using only the information given in the context.\\n        If the context doesn't contain enough information to answer the question fully, state that clearly. \\n        Do not make up or infer information beyond what's explicitly stated in the context.\\n\\n        [INSTRUCTIONS]\\n        1. Carefully read the provided context and query.\\n        2. Analyze the information in the context that is relevant to the query.\\n        3. Formulate a clear and concise answer based solely on the given context.\\n        4. If the context doesn't provide sufficient information to answer the query, state this explicitly.\\n        5. Do not include any information that is not present in the given context.\\n    <|eot_id|>\\n    <|start_header_id|>\\n        user\\n    <|end_header_id|>\\n    Answer the user question based on the context provided below\\n\\n    [CONTEXT]:\\n    Compared to conventional discrete representations (e.g., the one-hot encoding), word embedding provides more robust representations for words, particulary for those that infrequently appear in the training data. More importantly, the embedding encodessyntactic and semantic content implicitly, so that relations among words can be simply computed as the distances among their embeddings, or word vectors.\\n\\nFollowing the idea that the meaning of a word can be determined by the company it keeps (Baroni and Zamparelli, 2010), i.e., the words that it co-occurs with, word embedding projects discrete words to a low-dimensional and continuous vector space where co-occurred words are located close to each other.\\n\\n1 Introduction Understanding the meaning of a word is at the heart of natural language processing (NLP). While a deep, human-like, understanding remains elusive, many methods have been successful in recovering certain aspects of similarity between words. Recently, neural-network based approaches in which words are embedded into a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008).\\n\\n1. Introduction In recent years, (monolingual) vector representations of words, so-called word embeddings (Mikolov, Chen, Corrado, & Dean, 2013a; Pennington, Socher, & Manning, 2014) have proven extremely useful across a wide range of natural language processing (NLP) applications.\\n\\nLetXR|V|dbe a word embedding matrix that is learned for the -th ofLlanguages whereVis the corresponding vocabulary and dis the dimensionality of the word embeddings. We will furthermore refer to the word embedding of the i-th word in language with the shorthand x iorxiif languageis clear from context. We will refer to the word corresponding to thei-th word embedding xiaswiwherewiis a string.\\n    \\n    [QUERY]\\n    explain word embeddings\\n    \\n    <|eot_id|>\\n    <|start_header_id|>\\n        assistant\\n    <|end_header_id|>\"}),\n",
       "  'llm': ComponentIntermediates(inputs={'messages': \"<|begin_of_text|>\\n    <|start_header_id|>\\n        system\\n    <|end_header_id|>\\n        You are an AI assistant tasked with answering questions based on provided context.\\n        Your goal is to provide accurate, relevant, and concise responses using only the information given in the context.\\n        If the context doesn't contain enough information to answer the question fully, state that clearly. \\n        Do not make up or infer information beyond what's explicitly stated in the context.\\n\\n        [INSTRUCTIONS]\\n        1. Carefully read the provided context and query.\\n        2. Analyze the information in the context that is relevant to the query.\\n        3. Formulate a clear and concise answer based solely on the given context.\\n        4. If the context doesn't provide sufficient information to answer the query, state this explicitly.\\n        5. Do not include any information that is not present in the given context.\\n    <|eot_id|>\\n    <|start_header_id|>\\n        user\\n    <|end_header_id|>\\n    Answer the user question based on the context provided below\\n\\n    [CONTEXT]:\\n    Compared to conventional discrete representations (e.g., the one-hot encoding), word embedding provides more robust representations for words, particulary for those that infrequently appear in the training data. More importantly, the embedding encodessyntactic and semantic content implicitly, so that relations among words can be simply computed as the distances among their embeddings, or word vectors.\\n\\nFollowing the idea that the meaning of a word can be determined by the company it keeps (Baroni and Zamparelli, 2010), i.e., the words that it co-occurs with, word embedding projects discrete words to a low-dimensional and continuous vector space where co-occurred words are located close to each other.\\n\\n1 Introduction Understanding the meaning of a word is at the heart of natural language processing (NLP). While a deep, human-like, understanding remains elusive, many methods have been successful in recovering certain aspects of similarity between words. Recently, neural-network based approaches in which words are embedded into a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008).\\n\\n1. Introduction In recent years, (monolingual) vector representations of words, so-called word embeddings (Mikolov, Chen, Corrado, & Dean, 2013a; Pennington, Socher, & Manning, 2014) have proven extremely useful across a wide range of natural language processing (NLP) applications.\\n\\nLetXR|V|dbe a word embedding matrix that is learned for the -th ofLlanguages whereVis the corresponding vocabulary and dis the dimensionality of the word embeddings. We will furthermore refer to the word embedding of the i-th word in language with the shorthand x iorxiif languageis clear from context. We will refer to the word corresponding to thei-th word embedding xiaswiwherewiis a string.\\n    \\n    [QUERY]\\n    explain word embeddings\\n    \\n    <|eot_id|>\\n    <|start_header_id|>\\n        assistant\\n    <|end_header_id|>\"}, outputs={'output': ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=\"Word embeddings provide more robust representations for words, particularly for those that infrequently appear in the training data. They implicitly capture syntactic and semantic content, allowing relations among words to be computed as the distances among their embeddings (or word vectors). This is based on the idea that a word's meaning can be determined by the company it keeps (i.e., the words it co-occurs with), projecting discrete words into a low-dimensional and continuous vector space where co-occurred words are located close to each other.\", additional_kwargs={'tool_calls': []}), raw={'model': 'llama3.1:latest', 'created_at': '2024-09-03T17:56:53.311215Z', 'message': {'role': 'assistant', 'content': \"Word embeddings provide more robust representations for words, particularly for those that infrequently appear in the training data. They implicitly capture syntactic and semantic content, allowing relations among words to be computed as the distances among their embeddings (or word vectors). This is based on the idea that a word's meaning can be determined by the company it keeps (i.e., the words it co-occurs with), projecting discrete words into a low-dimensional and continuous vector space where co-occurred words are located close to each other.\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 10496362459, 'load_duration': 43518417, 'prompt_eval_count': 628, 'prompt_eval_duration': 6674073000, 'eval_count': 105, 'eval_duration': 3775575000}, delta=None, logprobs=None, additional_kwargs={})})})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "steps, response = rag_manager_hr.run_query_pipeline(\"explain word embeddings\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in steps[\"retriever\"].outputs[\"output\"]:\n",
    "#     print(f'text:{x.text}')\n",
    "#     print(f'id {x.id_}')\n",
    "#     print(f'page num {x.metadata[\"page_label\"]}, file_name {x.metadata[\"file_name\"]}')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(steps[\"llm\"].inputs[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all document name in Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -X GET \"http://127.0.0.1:5000/api/documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new document in Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -X POST \"http://127.0.0.1:5000/api/documents\" -F \"file=@/Users/ali/Desktop/Coding/projects/sciquery_advance_rag/data/pdf_documents/martins2016_sparsemax.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete new document from Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -X DELETE \"http://127.0.0.1:5000/api/documents/martins2016_sparsemax.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -X POST \"http://127.0.0.1:5000/api/query\" -H \"Content-Type: application/json\" -d '{\"query\": \"Explain what is ULMFIT?\"}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unstructured_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

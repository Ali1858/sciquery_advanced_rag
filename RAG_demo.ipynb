{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciQuery: Advanced RAG System using LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ali/miniforge3/envs/sciquery_p310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from app.run_pipeline_manager import RAGPipelineManager\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with semantic chunking and fusion retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== LLM llama3.1:latest is loaded successfully using the provider ollama\n",
      "============================== Embedding mixedbread-ai/mxbai-embed-large-v1 is loaded successfully using the provider huggingface\n",
      "pipeline will parse document False, save vector False and run injestion False.\n",
      "data/pdf_documents\n",
      "Not parsing pdf document. Its already in doc store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Vector Store created using vector db qdrant\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAGPipelineManager\n",
    "rag_manager_semantic = RAGPipelineManager(document_parsing_method=\"simple\",node_parsing_method=\"semantic\", retrieval_type = \"fusion_retrieval\")\n",
    "# Create the RAG pipeline\n",
    "rag_manager_semantic.create_query_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. However, it is revealed that much of this performance gain is due to certain system design choices and hyperparameter optimizations rather than the embedding algorithms themselves. These modications can be transferred to traditional distributional models, yielding similar gains. The context also highlights that global advantage to any single approach over others is mostly local or insignificant.\n"
     ]
    }
   ],
   "source": [
    "output, steps = rag_manager_semantic.run_query_pipeline(\"explain word embeddings?\")\n",
    "print(output.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with semantic chunk and simple retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== LLM llama3.1:latest is loaded successfully using the provider ollama\n",
      "============================== Embedding mixedbread-ai/mxbai-embed-large-v1 is loaded successfully using the provider huggingface\n",
      "pipeline will parse document False, save vector False and run injestion False.\n",
      "data/pdf_documents\n",
      "Not parsing pdf document. Its already in doc store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Vector Store created using vector db qdrant\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAGPipelineManager\n",
    "rag_manager_semantic_simple = RAGPipelineManager(document_parsing_method=\"simple\",node_parsing_method=\"semantic\", retrieval_type = \"simple\")\n",
    "# Create the RAG pipeline\n",
    "rag_manager_semantic_simple.create_query_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, word embeddings represent words as a d-dimensional vector of real numbers in a low-dimensional space, where vectors that are close to each other are shown to be semantically related. This means that words with similar meanings or contexts will have similar vector representations.\n",
      "\n",
      "The context also explains that word embeddings project discrete words to a continuous vector space, allowing for more robust representations of words, especially for those that infrequently appear in the training data. Additionally, it mentions that word vectors learned by log-linear models can exhibit linear relations among relevant words, which can be computed by simple vector addition and subtraction.\n",
      "\n",
      "Examples of linear relations mentioned in the context include \"Paris France + Rome = Italy\" and the concept of bilingual word translation tasks where a linear transform is used to project semantically identical words from one language to another.\n"
     ]
    }
   ],
   "source": [
    "output, steps = rag_manager_semantic_simple.run_query_pipeline(\"explain word embeddings\")\n",
    "print(output.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with sentence window chunking and small-to-big retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== LLM llama3.1:latest is loaded successfully using the provider ollama\n",
      "============================== Embedding mixedbread-ai/mxbai-embed-large-v1 is loaded successfully using the provider huggingface\n",
      "pipeline will parse document False, save vector False and run injestion False.\n",
      "data/pdf_documents\n",
      "Not parsing pdf document. Its already in doc store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Vector Store created using vector db qdrant\n",
      "Adding function to fetch window for sentence window small-to-big retrieval\n"
     ]
    }
   ],
   "source": [
    "rag_manager_sent_window = RAGPipelineManager(document_parsing_method=\"simple\",node_parsing_method=\"sentence_window\", retrieval_type=\"small_to_big\")\n",
    "\n",
    "# Create the RAG pipelines\n",
    "rag_manager_sent_window.create_query_pipeline(verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here's a concise explanation of word embeddings:\n",
      "\n",
      "Word embeddings are vector representations of words that capture their semantic and syntactic information. They enable comparison of word meanings across languages, which is useful for tasks such as bilingual lexicon induction, machine translation, and cross-lingual information retrieval. Word embeddings are created by optimizing for the same objectives, with seemingly different models often being equivalent due to variations in optimization strategies, hyperparameters, and other factors.\n",
      "\n",
      "These representations can be used in various NLP applications, including named entity recognition (NER) and chunking tasks. The context also mentions that word embeddings reflect how language users organize concepts, and their similarity can indicate near-isomorphic structures across languages. However, the paper questions whether non-isomorphism is a sign of degenerate word vector spaces.\n",
      "\n",
      "Word embeddings are often learned along with other network parameters using backpropagation and can be used in conjunction with recurrent networks to produce context-sensitive representations. They have been shown to alleviate sparsity issues in supervised learning setups and are widely used in NLP systems due to their simplicity and efficacy.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output, steps = rag_manager_sent_window.run_query_pipeline(\"explain word embeddings\")\n",
    "print(output.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(steps[\"llm\"].inputs[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with hierarchical chunking and small-to-big retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== LLM llama3.1:latest is loaded successfully using the provider ollama\n",
      "============================== Embedding mixedbread-ai/mxbai-embed-large-v1 is loaded successfully using the provider huggingface\n",
      "pipeline will parse document False, save vector False and run injestion False.\n",
      "data/pdf_documents\n",
      "Not parsing pdf document. Its already in doc store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Vector Store created using vector db qdrant\n"
     ]
    }
   ],
   "source": [
    "rag_manager_hr = RAGPipelineManager(document_parsing_method=\"simple\",node_parsing_method=\"hierarchical\", retrieval_type=\"small_to_big\")\n",
    "\n",
    "# Create the RAG pipeline\n",
    "rag_manager_hr.create_query_pipeline(verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, word embeddings provide more robust representations for words, particularly for those that infrequently appear in training data. They implicitly encode both syntactic and semantic content, allowing relations among words to be computed as distances among their embeddings. Word embeddings project discrete words into a low-dimensional and continuous vector space where co-occurred words are located close to each other.\n",
      "\n",
      "In simpler terms, word embeddings represent words as vectors in a high-dimensional space where semantically similar words (i.e., those that often appear together) are positioned near each other, enabling efficient computation of relationships between words.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output, steps = rag_manager_hr.run_query_pipeline(\"explain word embeddings\")\n",
    "print(output.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:Compared to conventional discrete representations (e.g., the one-hot encoding), word embedding provides more robust representations for words, particulary for those that infrequently appear in the training data. More importantly, the embedding encodessyntactic and semantic content implicitly, so that relations among words can be simply computed as the distances among their embeddings, or word vectors.\n",
      "id 024adeeb-8098-4ad5-8075-5ca258438d03\n",
      "page num 1006, file_name xing2015_normalized_we_orthogonal_transform.pdf\n",
      "\n",
      "text:Following the idea that the meaning of a word can be determined by the company it keeps (Baroni and Zamparelli, 2010), i.e., the words that it co-occurs with, word embedding projects discrete words to a low-dimensional and continuous vector space where co-occurred words are located close to each other.\n",
      "id ae76a1a6-bcd2-48f7-a236-65500506b5a4\n",
      "page num 1006, file_name xing2015_normalized_we_orthogonal_transform.pdf\n",
      "\n",
      "text:1 Introduction Understanding the meaning of a word is at the heart of natural language processing (NLP). While a deep, human-like, understanding remains elusive, many methods have been successful in recovering certain aspects of similarity between words. Recently, neural-network based approaches in which words are embedded into a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008).\n",
      "id 223ad6c7-3c2d-471b-a068-f6fc0f39f1c1\n",
      "page num 1, file_name levy2015_improving_dist_sim.pdf\n",
      "\n",
      "text:1. Introduction In recent years, (monolingual) vector representations of words, so-called word embeddings (Mikolov, Chen, Corrado, & Dean, 2013a; Pennington, Socher, & Manning, 2014) have proven extremely useful across a wide range of natural language processing (NLP) applications.\n",
      "id 56b1fe72-c1e9-4c1e-a376-4e12889ce8f7\n",
      "page num 1, file_name ruder2019_survey_of_cross_lingual_we_odels.pdf\n",
      "\n",
      "text:LetXR|V|dbe a word embedding matrix that is learned for the -th ofLlanguages whereVis the corresponding vocabulary and dis the dimensionality of the word embeddings. We will furthermore refer to the word embedding of the i-th word in language with the shorthand x iorxiif languageis clear from context. We will refer to the word corresponding to thei-th word embedding xiaswiwherewiis a string.\n",
      "id af3e899a-8c3c-4e5c-8001-f9207e48afd6\n",
      "page num 3, file_name ruder2019_survey_of_cross_lingual_we_odels.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in steps[\"retriever\"].outputs[\"output\"]:\n",
    "    print(f'text:{x.text}')\n",
    "    print(f'id {x.id_}')\n",
    "    print(f'page num {x.metadata[\"page_label\"]}, file_name {x.metadata[\"file_name\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(steps[\"llm\"].inputs[\"messages\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unstructured_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciQuery: Advanced RAG System using LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ali/miniforge3/envs/sciquery_p310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "\n",
    "from app.llama_index_utils import *\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from app.llama_index_utils import create_vector_collection, load_vector_collection, prepare_document\n",
    "from config import QDRANT_COLLECTION_NAME\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Document using pdf files\n",
    "\n",
    "We have two options, one, using `llama_index's` pdf parser and second is manual parsing using `pymupdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "method = \"simple\"#\"manual_parsing\"\n",
    "pdf_paths = \"data/pdf_documents\"\n",
    "\n",
    "documents = prepare_document(pdf_paths=pdf_paths,method=method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM and Embedding\n",
    "\n",
    "We load LLAMA 3.1 8b using Ollama and embedding is loaded from sentence transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== LLM llama3.1:latest is loaded successfully using the provider ollama\n",
      "============================== Embedding mixedbread-ai/mxbai-embed-large-v1 is loaded successfully using the provider huggingface\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Settings.llm = get_llms()\n",
    "Settings.embed_model = get_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This step convert documents to nodes/chunks\n",
    "\n",
    "Using Sementic Splitter we group by sentence which are sementically similar and create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 2614)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_node_parser = get_node_parser(Settings.embed_model, parsing_method=\"semantic\")\n",
    "\n",
    "transforms = [semantic_node_parser,Settings.embed_model]\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=transforms)\n",
    "\n",
    "# nodes = pipeline.run(nodes=documents)\n",
    "# len(documents), len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist nodes as a Document store on disk\n",
    "and later load it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_store_name = \"semantic-document-nodes\"\n",
    "# save_simple_doc_store(nodes,doc_store_name)\n",
    "\n",
    "nodes_ = get_doc_store(doc_store_name)\n",
    "nodes = list(nodes_.docs.values())\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embedding for each nodes\n",
    "\n",
    "and index and persist it using Qdrant Vector Store.\n",
    "later load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Vectore Store created using vector db qdrant\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x17d4b1ba0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create_vector_collection(nodes, \"semantic-document-collection\")\n",
    "index = load_vector_collection(\"semantic-document-collection\", embed_model=Settings.embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# generate question regarding topic\n",
    "prompt_str1 = \"Represent this sentence for searching relevant passages: {topic}\"\n",
    "prompt_tmpl1 = PromptTemplate(prompt_str1)\n",
    "\n",
    "prompt_str2 = \"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>\n",
    "        system\n",
    "    <|end_header_id|>\n",
    "        You are an AI assistant tasked with answering questions based on provided context.\n",
    "        Your goal is to provide accurate, relevant, and concise responses using only the information given in the context.\n",
    "        If the context doesn't contain enough information to answer the question fully, state that clearly. \n",
    "        Do not make up or infer information beyond what's explicitly stated in the context.\n",
    "\n",
    "        [INSTRUCTIONS]\n",
    "        1. Carefully read the provided context and query.\n",
    "        2. Analyze the information in the context that is relevant to the query.\n",
    "        3. Formulate a clear and concise answer based solely on the given context.\n",
    "        4. If the context doesn't provide sufficient information to answer the query, state this explicitly.\n",
    "        5. Do not include any information that is not present in the given context.\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>\n",
    "        user\n",
    "    <|end_header_id|>\n",
    "    Answer the user question based on the context provided below\n",
    "\n",
    "    [CONTEXT]:\n",
    "    {context_str}\n",
    "    \n",
    "    [QUERY]\n",
    "    {input}\n",
    "    \n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>\n",
    "        assistant\n",
    "    <|end_header_id|>\n",
    "    \"\"\"\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG using Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import InputComponent, FunctionComponent\n",
    "\n",
    "# Function to concatenate retrieved nodes\n",
    "def concatenate_nodes(retrieved_nodes):\n",
    "    return \"\\n\\n\".join([node.node.get_content() for node in retrieved_nodes])\n",
    "\n",
    "# Create a FunctionComponent for concatenation\n",
    "concat_component = FunctionComponent(concatenate_nodes)\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "query_pipeline = QueryPipeline(\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "query_pipeline.add_modules(\n",
    "    {\n",
    "        \"input\":InputComponent(),\n",
    "        \"prompt_tmpl1\":prompt_tmpl1,\n",
    "        \"retriever\":retriever,\n",
    "        \"concat_component\": concat_component,\n",
    "        \"prompt_tmpl2\":prompt_tmpl2,\n",
    "        \"llm\":Settings.llm\n",
    "    }\n",
    ")\n",
    "\n",
    "query_pipeline.add_link(\"input\",\"prompt_tmpl1\",dest_key=\"topic\")\n",
    "query_pipeline.add_link(\"prompt_tmpl1\",\"retriever\")\n",
    "query_pipeline.add_link(\"retriever\",\"concat_component\")\n",
    "query_pipeline.add_link(\"input\",\"prompt_tmpl2\",dest_key=\"input\")\n",
    "query_pipeline.add_link(\"concat_component\",\"prompt_tmpl2\",dest_key=\"context_str\")\n",
    "query_pipeline.add_link(\"prompt_tmpl2\",\"llm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "topic: What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module prompt_tmpl1 with input: \n",
      "topic: What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module retriever with input: \n",
      "input: Represent this sentence for searching relevant passages: What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are ...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module concat_component with input: \n",
      "retrieved_nodes: [NodeWithScore(node=TextNode(id_='6f27def0-4877-4535-aac5-001f41acc5d1', embedding=None, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_document...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module prompt_tmpl2 with input: \n",
      "input: What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?\n",
      "context_str: 2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly low...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
      "messages: \n",
      "    <|begin_of_text|>\n",
      "    <|start_header_id|>\n",
      "        system\n",
      "    <|end_header_id|>\n",
      "        You are an AI assistant tasked with answering questions based on provided context.\n",
      "        Your goal is to p...\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='According to the context, one limitation of using mBERT for zero-shot cross-lingual transfer is that it can be hindered by the \"curse of multilinguality\", where adding more pretraining languages up to a point hurts downstream transfer. This effect can be mitigated by increasing model capacity or additional training for particular language pairs.\\n\\nAdditionally, when new scripts or languages are introduced, mBERT may not have been specifically trained on these languages, which can lead to decreased performance. The text mentions that mBERT is pre-trained on the 104 languages with the largest Wikipedias, implying that languages without a large Wikipedia presence may not be well-represented in the model.\\n\\nThere is also a mention of the \"overt-fitting\" issue when training models for zero-shot transfer, where the model overfits to the original language rather than generalizing to new languages. This can make it difficult to ensure that the probing model did not overfit to the original language.\\n\\nThe text does not specify how these limitations affect performance specifically, but it implies that there are challenges associated with using mBERT for zero-shot cross-lingual transfer when working with new scripts or languages.', additional_kwargs={'tool_calls': []}), raw={'model': 'llama3.1:latest', 'created_at': '2024-08-25T19:03:54.707853Z', 'message': {'role': 'assistant', 'content': 'According to the context, one limitation of using mBERT for zero-shot cross-lingual transfer is that it can be hindered by the \"curse of multilinguality\", where adding more pretraining languages up to a point hurts downstream transfer. This effect can be mitigated by increasing model capacity or additional training for particular language pairs.\\n\\nAdditionally, when new scripts or languages are introduced, mBERT may not have been specifically trained on these languages, which can lead to decreased performance. The text mentions that mBERT is pre-trained on the 104 languages with the largest Wikipedias, implying that languages without a large Wikipedia presence may not be well-represented in the model.\\n\\nThere is also a mention of the \"overt-fitting\" issue when training models for zero-shot transfer, where the model overfits to the original language rather than generalizing to new languages. This can make it difficult to ensure that the probing model did not overfit to the original language.\\n\\nThe text does not specify how these limitations affect performance specifically, but it implies that there are challenges associated with using mBERT for zero-shot cross-lingual transfer when working with new scripts or languages.'}, 'done_reason': 'stop', 'done': True, 'total_duration': 35070166458, 'load_duration': 14294382375, 'prompt_eval_count': 2239, 'prompt_eval_duration': 10185746000, 'eval_count': 237, 'eval_duration': 10587653000}, delta=None, logprobs=None, additional_kwargs={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Explain what is ULMFIT?\"\n",
    "\n",
    "output, intermediates = query_pipeline.run_with_intermediates(topic=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': ComponentIntermediates(inputs={'topic': 'What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?'}, outputs={'topic': 'What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?'}),\n",
       " 'prompt_tmpl1': ComponentIntermediates(inputs={'topic': 'What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?'}, outputs={'prompt': 'Represent this sentence for searching relevant passages: What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?'}),\n",
       " 'retriever': ComponentIntermediates(inputs={'input': 'Represent this sentence for searching relevant passages: What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?'}, outputs={'output': [NodeWithScore(node=TextNode(id_='6f27def0-4877-4535-aac5-001f41acc5d1', embedding=None, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='490cf48f-97c9-4215-a71e-187753c9eeb1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='12e77fb0ab491f39fd6abb121c270e3887a2a0270e90cbb9a301f09618bb2626'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='50367415-5691-4eb4-aca3-577b65f7a4e8', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='5c519a54684e934fd33ecf31e2d6154ba8ef87667abd8932588e65bb3e35a7d5')}, text='2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly lower computational cost.Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions regarding how well the cross-lingual mBERT representation captures lexical semantics, motivating our work. In this paper, we directly assess the cross-lingual properties of multilingual representations on tasks where lexical semantics plays an important role and present one unsuccessful and two successful methods for achieving better language neutrality. Multilingual capabilities of representations are often evaluated by zero-shot transfer from the training language to a test language (Hu et al., 2020; Liang et al., 2020). However, in such a setup, we can never be sure if the probing model did not overt for the original language, as training is usually stopped when accuracy decreases on a validation set from the same language (otherwise, it would not be zero-shot), even when it would have been better to stop the training earlier. This overtting on the original language can pose a disadvantage for information-richer representations. To avoid such methodological issues, we select tasks that only involve a direct comparison of the representations with no training: cross-lingual sentence retrieval, word alignment (WA), and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language ID classiers and assessing how the representation similarity corresponds to phylogenetic language families. We nd that contextual representations are more language-neutral than static word embeddings which have been explicitly trained to represent matching words similarly and can be used in a', mimetype='text/plain', start_char_idx=2037, end_char_idx=4172, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8297281), NodeWithScore(node=TextNode(id_='3a121c11-fa9c-495b-85f8-e79b55fad7a5', embedding=None, metadata={'page_label': '1', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='762a1ca6-7729-440c-86c8-a1ffc0242596', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='2e40bf108e042f9985f3c7f6b071c59890907d489f20a12448490e92003477f2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2c695a8d-1d05-4d7b-9b7d-c26bff07ea16', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='bef3f6d88b213b02ee2ec374796b8fd749a752a8731338fdaa7f3d750bb4c6db')}, text='Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, November 37, 2019. c2019 Association for Computational Linguistics833Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT Shijie Wu and Mark Dredze Department of Computer Science Johns Hopkins University shijie.wu@jhu.edu, mdredze@cs.jhu.edu Abstract Pretrained contextual representation models (Peters et al. ,2018 ;Devlin et al. ,2019 ) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT ( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classication, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and nd mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specic features, and measure factors that inuence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks ( Peters et al. ,2018 ;Howard and Ruder ,2018 ;Radford et al. ', mimetype='text/plain', start_char_idx=0, end_char_idx=1629, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8261702), NodeWithScore(node=TextNode(id_='dfbfa344-e9cb-466c-af87-1814ea8b8d9d', embedding=None, metadata={'page_label': '8', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='36ef6974-c094-4fc1-9cc8-b9d642e05b81', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '8', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='acbc99f58f0de90fc67abae1bb5bfca19f132e244f5ee98544552ad635f0d968'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a8d31636-6e29-4e96-9476-9f2594b323fc', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='ed1c43d067d052074c9c014f64c1ff1f62bafeb7b5f55a2060834ca15af4f186')}, text='Languages are sorted by mBERT zero-shot transfer performance. Three downward triangles indicate performance drop more than the legends lower limit. 96.396.496.596.696.796.896.9 0 2 4 6 8 10 12 LayerAccuracy Figure 2: Language identication accuracy for different layer of mBERT. layer 0 is the embedding layer and the layeri >0is output of the ithtransformer block. across languages in any task, we assume Ven trainis the set of all subwords in the English training set, V testis the set of all subwords in language test set, andc wis the count of subword win test set of language . We then calculate the percentage of observed subwords at type-level p typeand token-levelp token for each target language . p type=|V obs| |Vtest|100 p token= wV obsc w wV testcw100 whereV obs=Ven trainV test. In Fig. 3, we show the relation between crosslingual zero-shot transfer performance of mBERT andp typeorp token for all ve tasks with Pearson correlation. In four out of ve tasks (not XNLI) we observed a strong positive correlation ( p <0.05) with a correlation coefcient larger than 0.5. In Indo-European languages, we observed p token is usually around 50% to 75% while p typeis usually less than 50%. This indicates that subwords shared across languages are usually high frequency6. We 6With the data-dependent WordPiece algorithm, subwords that appear in multiple languages with high frequency are more likely to be selected.', mimetype='text/plain', start_char_idx=2844, end_char_idx=4265, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.82600117), NodeWithScore(node=TextNode(id_='f2c48d65-3474-4e2e-a745-f0eb35270402', embedding=None, metadata={'page_label': '3', 'file_name': 'lauscher2020_fromzero_to_hero.pdf', 'file_path': 'data/pdf_documents/lauscher2020_fromzero_to_hero.pdf', 'file_type': 'application/pdf', 'file_size': 4938400, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='cbc5a191-ffa4-441f-8b7e-a76fdf6ce5d9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'lauscher2020_fromzero_to_hero.pdf', 'file_path': 'data/pdf_documents/lauscher2020_fromzero_to_hero.pdf', 'file_type': 'application/pdf', 'file_size': 4938400, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='3cdd53269810ac23e550e5641dd34ee592bca6e7f3d60139fe4b66765a6a6fe9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='55dda3dc-d11b-402c-970b-3d347412109b', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '3', 'file_name': 'lauscher2020_fromzero_to_hero.pdf', 'file_path': 'data/pdf_documents/lauscher2020_fromzero_to_hero.pdf', 'file_type': 'application/pdf', 'file_size': 4938400, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='f33cb84b50db5374e10c3b084e6f1ad776866b010897b2869199811faa6aa69d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6fa105c7-57e1-4714-a934-f4f31f3d055b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6d64cd426f2bc86ae25b91c6e5c9e1c5edacc7fdeced18abfc0d340ccbcae992')}, text='For XLM-R, Conneau et al. (2020) observe that for a xed model capacity, downstream cross-lingual transfer improves with more pretraining languages up to a point after which adding more pretraining languages hurts downstream transfer. This effect, termed the curse of multilinguality, can be mitigated by increasing models capacity (Artetxe et al., 2020b) or additional training for particular language pairs (Pfeiffer et al., 2020). This points to MMTs capacity (i.e., computational budgets), as a critical factor for effective zero-shot transfer. In contrast, we identify few-shot transfer as a much more cost-effective strategy for improving downstream target language performance ( 4). We show for a number of target languages and downstream tasks, that one can obtain large performance gains at very small annotation cost, without having to pretrain from scratch an MMT of larger capacity. 2.3 Cross-Lingual Transfer with MMTs A body of recent work probed the knowledge encoded in MMTs, primarily mBERT. Libovick `y et al. (2020) analyze language-specic versus languageuniversal knowledge encoded in mBERT. Pires et al. (2019) demonstrate mBERT to be effective for POS-tagging and NER zero-shot transfer between related languages. Wu and Dredze (2019) extend this analysis to more tasks and languages, and show that mBERT-based transfer is on a par with the best task-specic zero-shot transfer approaches. Similarly, Karthikeyan et al. (2020) prove mBERT to be effective for NER and NLI transfer to Hindi, Spanish, and Russian.1Importantly, they show that transfer effectiveness does not depend on the vocabulary overlap between the languages. In most recent work, concurrent to this, Hu et al. (2020) introduce XTREME, a benchmark for eval1Note that all three are high-resource Indo-European languages with large Wikipedias.uating multilingual encoders encompassing 9 tasks and 40 languages.2While the primary focus is a large-scale zero-shot transfer evaluation, they also experiment with target-language ne-tuning (1,000 instances for POS and NER). While Hu et al. (2020) focus on the evaluation aspects and protocols, in this work, we provide a more detailed analysis of the factors that hinder effective zeroshot transfer across several tasks.3We also put more emphasis on few-shot transfer, and approach it differently: by sequentially ne-tuning MMTs, rst on (larger) source language training data and then on few target-language instances. Artetxe et al. ', mimetype='text/plain', start_char_idx=522, end_char_idx=2988, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8252075), NodeWithScore(node=TextNode(id_='d9650908-6491-488d-8744-1816fe9de667', embedding=None, metadata={'page_label': '6', 'file_name': 'pfeiffer2021_adapting_multilingual_lms_to_new_scripts_unks.pdf', 'file_path': 'data/pdf_documents/pfeiffer2021_adapting_multilingual_lms_to_new_scripts_unks.pdf', 'file_type': 'application/pdf', 'file_size': 661550, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='375ee4a8-5b9f-4fc1-ba59-7c94333fafa6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'pfeiffer2021_adapting_multilingual_lms_to_new_scripts_unks.pdf', 'file_path': 'data/pdf_documents/pfeiffer2021_adapting_multilingual_lms_to_new_scripts_unks.pdf', 'file_type': 'application/pdf', 'file_size': 661550, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='9dd218260bbb590553092ebc54256d8e8c13b303cc4eea7212cf5d7b867d313a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='47077cf4-e243-4c9b-94d9-6188ee2e19b3', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='cd9f258beec54930a4f98e45506cc0a293e3d24119def2f4d6b06768de323d1d')}, text='10191Languages. WikiAnn offers a wide language coverage (176 languages) and, consequently, a number of language-related comparative analyses. In order to systematically compare against state-of-the-art cross-lingual methods under different evaluation conditions, we identify 1)low-resource languages where the script has been covered by mBERT but the model has not been specically trained on the language 2)as well as low-resource languages with scripts not covered at all by pretraining. In each case, we select four languages, taking into account variance in data availability and typological diversity. We select four high-resource source languages (English, Chinese, Japanese, Arabic) in order to go beyond English-centric transfer. We evaluate the cross-lingual transfer performance of these 4 languages to the 17 diverse languages. For DP, we chose the subset that occurs in UD. We highlight the properties of all 21 languages in Table 1. 4.1 Baselines mBERT (Standard Transfer Setup). We primarily focus on mBERT as it has been shown to work well for low-resource languages (Pfeiffer et al., 2020b). mBERT is trained on the 104 languages with the largest Wikipedias.6In the standard crosslingual transfer setting (see 2), the full model is ne-tuned on the target task in the (high-resource) source language, and is evaluated on the test set of the target (low-resource) language. MAD-X. ', mimetype='text/plain', start_char_idx=0, end_char_idx=1394, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8176185)]}),\n",
       " 'concat_component': ComponentIntermediates(inputs={'retrieved_nodes': [NodeWithScore(node=TextNode(id_='6f27def0-4877-4535-aac5-001f41acc5d1', embedding=None, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='490cf48f-97c9-4215-a71e-187753c9eeb1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='12e77fb0ab491f39fd6abb121c270e3887a2a0270e90cbb9a301f09618bb2626'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='50367415-5691-4eb4-aca3-577b65f7a4e8', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='5c519a54684e934fd33ecf31e2d6154ba8ef87667abd8932588e65bb3e35a7d5')}, text='2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly lower computational cost.Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions regarding how well the cross-lingual mBERT representation captures lexical semantics, motivating our work. In this paper, we directly assess the cross-lingual properties of multilingual representations on tasks where lexical semantics plays an important role and present one unsuccessful and two successful methods for achieving better language neutrality. Multilingual capabilities of representations are often evaluated by zero-shot transfer from the training language to a test language (Hu et al., 2020; Liang et al., 2020). However, in such a setup, we can never be sure if the probing model did not overt for the original language, as training is usually stopped when accuracy decreases on a validation set from the same language (otherwise, it would not be zero-shot), even when it would have been better to stop the training earlier. This overtting on the original language can pose a disadvantage for information-richer representations. To avoid such methodological issues, we select tasks that only involve a direct comparison of the representations with no training: cross-lingual sentence retrieval, word alignment (WA), and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language ID classiers and assessing how the representation similarity corresponds to phylogenetic language families. We nd that contextual representations are more language-neutral than static word embeddings which have been explicitly trained to represent matching words similarly and can be used in a', mimetype='text/plain', start_char_idx=2037, end_char_idx=4172, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8297281), NodeWithScore(node=TextNode(id_='3a121c11-fa9c-495b-85f8-e79b55fad7a5', embedding=None, metadata={'page_label': '1', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='762a1ca6-7729-440c-86c8-a1ffc0242596', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='2e40bf108e042f9985f3c7f6b071c59890907d489f20a12448490e92003477f2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2c695a8d-1d05-4d7b-9b7d-c26bff07ea16', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='bef3f6d88b213b02ee2ec374796b8fd749a752a8731338fdaa7f3d750bb4c6db')}, text='Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, November 37, 2019. c2019 Association for Computational Linguistics833Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT Shijie Wu and Mark Dredze Department of Computer Science Johns Hopkins University shijie.wu@jhu.edu, mdredze@cs.jhu.edu Abstract Pretrained contextual representation models (Peters et al. ,2018 ;Devlin et al. ,2019 ) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT ( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classication, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and nd mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specic features, and measure factors that inuence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks ( Peters et al. ,2018 ;Howard and Ruder ,2018 ;Radford et al. ', mimetype='text/plain', start_char_idx=0, end_char_idx=1629, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8261702), NodeWithScore(node=TextNode(id_='dfbfa344-e9cb-466c-af87-1814ea8b8d9d', embedding=None, metadata={'page_label': '8', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='36ef6974-c094-4fc1-9cc8-b9d642e05b81', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '8', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='acbc99f58f0de90fc67abae1bb5bfca19f132e244f5ee98544552ad635f0d968'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a8d31636-6e29-4e96-9476-9f2594b323fc', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='ed1c43d067d052074c9c014f64c1ff1f62bafeb7b5f55a2060834ca15af4f186')}, text='Languages are sorted by mBERT zero-shot transfer performance. Three downward triangles indicate performance drop more than the legends lower limit. 96.396.496.596.696.796.896.9 0 2 4 6 8 10 12 LayerAccuracy Figure 2: Language identication accuracy for different layer of mBERT. layer 0 is the embedding layer and the layeri >0is output of the ithtransformer block. across languages in any task, we assume Ven trainis the set of all subwords in the English training set, V testis the set of all subwords in language test set, andc wis the count of subword win test set of language . We then calculate the percentage of observed subwords at type-level p typeand token-levelp token for each target language . p type=|V obs| |Vtest|100 p token= wV obsc w wV testcw100 whereV obs=Ven trainV test. In Fig. 3, we show the relation between crosslingual zero-shot transfer performance of mBERT andp typeorp token for all ve tasks with Pearson correlation. In four out of ve tasks (not XNLI) we observed a strong positive correlation ( p <0.05) with a correlation coefcient larger than 0.5. In Indo-European languages, we observed p token is usually around 50% to 75% while p typeis usually less than 50%. This indicates that subwords shared across languages are usually high frequency6. We 6With the data-dependent WordPiece algorithm, subwords that appear in multiple languages with high frequency are more likely to be selected.', mimetype='text/plain', start_char_idx=2844, end_char_idx=4265, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.82600117), NodeWithScore(node=TextNode(id_='f2c48d65-3474-4e2e-a745-f0eb35270402', embedding=None, metadata={'page_label': '3', 'file_name': 'lauscher2020_fromzero_to_hero.pdf', 'file_path': 'data/pdf_documents/lauscher2020_fromzero_to_hero.pdf', 'file_type': 'application/pdf', 'file_size': 4938400, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='cbc5a191-ffa4-441f-8b7e-a76fdf6ce5d9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'lauscher2020_fromzero_to_hero.pdf', 'file_path': 'data/pdf_documents/lauscher2020_fromzero_to_hero.pdf', 'file_type': 'application/pdf', 'file_size': 4938400, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='3cdd53269810ac23e550e5641dd34ee592bca6e7f3d60139fe4b66765a6a6fe9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='55dda3dc-d11b-402c-970b-3d347412109b', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '3', 'file_name': 'lauscher2020_fromzero_to_hero.pdf', 'file_path': 'data/pdf_documents/lauscher2020_fromzero_to_hero.pdf', 'file_type': 'application/pdf', 'file_size': 4938400, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='f33cb84b50db5374e10c3b084e6f1ad776866b010897b2869199811faa6aa69d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6fa105c7-57e1-4714-a934-f4f31f3d055b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6d64cd426f2bc86ae25b91c6e5c9e1c5edacc7fdeced18abfc0d340ccbcae992')}, text='For XLM-R, Conneau et al. (2020) observe that for a xed model capacity, downstream cross-lingual transfer improves with more pretraining languages up to a point after which adding more pretraining languages hurts downstream transfer. This effect, termed the curse of multilinguality, can be mitigated by increasing models capacity (Artetxe et al., 2020b) or additional training for particular language pairs (Pfeiffer et al., 2020). This points to MMTs capacity (i.e., computational budgets), as a critical factor for effective zero-shot transfer. In contrast, we identify few-shot transfer as a much more cost-effective strategy for improving downstream target language performance ( 4). We show for a number of target languages and downstream tasks, that one can obtain large performance gains at very small annotation cost, without having to pretrain from scratch an MMT of larger capacity. 2.3 Cross-Lingual Transfer with MMTs A body of recent work probed the knowledge encoded in MMTs, primarily mBERT. Libovick `y et al. (2020) analyze language-specic versus languageuniversal knowledge encoded in mBERT. Pires et al. (2019) demonstrate mBERT to be effective for POS-tagging and NER zero-shot transfer between related languages. Wu and Dredze (2019) extend this analysis to more tasks and languages, and show that mBERT-based transfer is on a par with the best task-specic zero-shot transfer approaches. Similarly, Karthikeyan et al. (2020) prove mBERT to be effective for NER and NLI transfer to Hindi, Spanish, and Russian.1Importantly, they show that transfer effectiveness does not depend on the vocabulary overlap between the languages. In most recent work, concurrent to this, Hu et al. (2020) introduce XTREME, a benchmark for eval1Note that all three are high-resource Indo-European languages with large Wikipedias.uating multilingual encoders encompassing 9 tasks and 40 languages.2While the primary focus is a large-scale zero-shot transfer evaluation, they also experiment with target-language ne-tuning (1,000 instances for POS and NER). While Hu et al. (2020) focus on the evaluation aspects and protocols, in this work, we provide a more detailed analysis of the factors that hinder effective zeroshot transfer across several tasks.3We also put more emphasis on few-shot transfer, and approach it differently: by sequentially ne-tuning MMTs, rst on (larger) source language training data and then on few target-language instances. Artetxe et al. ', mimetype='text/plain', start_char_idx=522, end_char_idx=2988, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8252075), NodeWithScore(node=TextNode(id_='d9650908-6491-488d-8744-1816fe9de667', embedding=None, metadata={'page_label': '6', 'file_name': 'pfeiffer2021_adapting_multilingual_lms_to_new_scripts_unks.pdf', 'file_path': 'data/pdf_documents/pfeiffer2021_adapting_multilingual_lms_to_new_scripts_unks.pdf', 'file_type': 'application/pdf', 'file_size': 661550, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='375ee4a8-5b9f-4fc1-ba59-7c94333fafa6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'pfeiffer2021_adapting_multilingual_lms_to_new_scripts_unks.pdf', 'file_path': 'data/pdf_documents/pfeiffer2021_adapting_multilingual_lms_to_new_scripts_unks.pdf', 'file_type': 'application/pdf', 'file_size': 661550, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='9dd218260bbb590553092ebc54256d8e8c13b303cc4eea7212cf5d7b867d313a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='47077cf4-e243-4c9b-94d9-6188ee2e19b3', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='cd9f258beec54930a4f98e45506cc0a293e3d24119def2f4d6b06768de323d1d')}, text='10191Languages. WikiAnn offers a wide language coverage (176 languages) and, consequently, a number of language-related comparative analyses. In order to systematically compare against state-of-the-art cross-lingual methods under different evaluation conditions, we identify 1)low-resource languages where the script has been covered by mBERT but the model has not been specically trained on the language 2)as well as low-resource languages with scripts not covered at all by pretraining. In each case, we select four languages, taking into account variance in data availability and typological diversity. We select four high-resource source languages (English, Chinese, Japanese, Arabic) in order to go beyond English-centric transfer. We evaluate the cross-lingual transfer performance of these 4 languages to the 17 diverse languages. For DP, we chose the subset that occurs in UD. We highlight the properties of all 21 languages in Table 1. 4.1 Baselines mBERT (Standard Transfer Setup). We primarily focus on mBERT as it has been shown to work well for low-resource languages (Pfeiffer et al., 2020b). mBERT is trained on the 104 languages with the largest Wikipedias.6In the standard crosslingual transfer setting (see 2), the full model is ne-tuned on the target task in the (high-resource) source language, and is evaluated on the test set of the target (low-resource) language. MAD-X. ', mimetype='text/plain', start_char_idx=0, end_char_idx=1394, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8176185)]}, outputs={'output': '2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly lower computational cost.Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions regarding how well the cross-lingual mBERT representation captures lexical semantics, motivating our work. In this paper, we directly assess the cross-lingual properties of multilingual representations on tasks where lexical semantics plays an important role and present one unsuccessful and two successful methods for achieving better language neutrality. Multilingual capabilities of representations are often evaluated by zero-shot transfer from the training language to a test language (Hu et al., 2020; Liang et al., 2020). However, in such a setup, we can never be sure if the probing model did not overt for the original language, as training is usually stopped when accuracy decreases on a validation set from the same language (otherwise, it would not be zero-shot), even when it would have been better to stop the training earlier. This overtting on the original language can pose a disadvantage for information-richer representations. To avoid such methodological issues, we select tasks that only involve a direct comparison of the representations with no training: cross-lingual sentence retrieval, word alignment (WA), and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language ID classiers and assessing how the representation similarity corresponds to phylogenetic language families. We nd that contextual representations are more language-neutral than static word embeddings which have been explicitly trained to represent matching words similarly and can be used in a\\n\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, November 37, 2019. c2019 Association for Computational Linguistics833Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT Shijie Wu and Mark Dredze Department of Computer Science Johns Hopkins University shijie.wu@jhu.edu, mdredze@cs.jhu.edu Abstract Pretrained contextual representation models (Peters et al. ,2018 ;Devlin et al. ,2019 ) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT ( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classication, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and nd mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specic features, and measure factors that inuence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks ( Peters et al. ,2018 ;Howard and Ruder ,2018 ;Radford et al. \\n\\nLanguages are sorted by mBERT zero-shot transfer performance. Three downward triangles indicate performance drop more than the legends lower limit. 96.396.496.596.696.796.896.9 0 2 4 6 8 10 12 LayerAccuracy Figure 2: Language identication accuracy for different layer of mBERT. layer 0 is the embedding layer and the layeri >0is output of the ithtransformer block. across languages in any task, we assume Ven trainis the set of all subwords in the English training set, V testis the set of all subwords in language test set, andc wis the count of subword win test set of language . We then calculate the percentage of observed subwords at type-level p typeand token-levelp token for each target language . p type=|V obs| |Vtest|100 p token= wV obsc w wV testcw100 whereV obs=Ven trainV test. In Fig. 3, we show the relation between crosslingual zero-shot transfer performance of mBERT andp typeorp token for all ve tasks with Pearson correlation. In four out of ve tasks (not XNLI) we observed a strong positive correlation ( p <0.05) with a correlation coefcient larger than 0.5. In Indo-European languages, we observed p token is usually around 50% to 75% while p typeis usually less than 50%. This indicates that subwords shared across languages are usually high frequency6. We 6With the data-dependent WordPiece algorithm, subwords that appear in multiple languages with high frequency are more likely to be selected.\\n\\nFor XLM-R, Conneau et al. (2020) observe that for a xed model capacity, downstream cross-lingual transfer improves with more pretraining languages up to a point after which adding more pretraining languages hurts downstream transfer. This effect, termed the curse of multilinguality, can be mitigated by increasing models capacity (Artetxe et al., 2020b) or additional training for particular language pairs (Pfeiffer et al., 2020). This points to MMTs capacity (i.e., computational budgets), as a critical factor for effective zero-shot transfer. In contrast, we identify few-shot transfer as a much more cost-effective strategy for improving downstream target language performance ( 4). We show for a number of target languages and downstream tasks, that one can obtain large performance gains at very small annotation cost, without having to pretrain from scratch an MMT of larger capacity. 2.3 Cross-Lingual Transfer with MMTs A body of recent work probed the knowledge encoded in MMTs, primarily mBERT. Libovick `y et al. (2020) analyze language-specic versus languageuniversal knowledge encoded in mBERT. Pires et al. (2019) demonstrate mBERT to be effective for POS-tagging and NER zero-shot transfer between related languages. Wu and Dredze (2019) extend this analysis to more tasks and languages, and show that mBERT-based transfer is on a par with the best task-specic zero-shot transfer approaches. Similarly, Karthikeyan et al. (2020) prove mBERT to be effective for NER and NLI transfer to Hindi, Spanish, and Russian.1Importantly, they show that transfer effectiveness does not depend on the vocabulary overlap between the languages. In most recent work, concurrent to this, Hu et al. (2020) introduce XTREME, a benchmark for eval1Note that all three are high-resource Indo-European languages with large Wikipedias.uating multilingual encoders encompassing 9 tasks and 40 languages.2While the primary focus is a large-scale zero-shot transfer evaluation, they also experiment with target-language ne-tuning (1,000 instances for POS and NER). While Hu et al. (2020) focus on the evaluation aspects and protocols, in this work, we provide a more detailed analysis of the factors that hinder effective zeroshot transfer across several tasks.3We also put more emphasis on few-shot transfer, and approach it differently: by sequentially ne-tuning MMTs, rst on (larger) source language training data and then on few target-language instances. Artetxe et al. \\n\\n10191Languages. WikiAnn offers a wide language coverage (176 languages) and, consequently, a number of language-related comparative analyses. In order to systematically compare against state-of-the-art cross-lingual methods under different evaluation conditions, we identify 1)low-resource languages where the script has been covered by mBERT but the model has not been specically trained on the language 2)as well as low-resource languages with scripts not covered at all by pretraining. In each case, we select four languages, taking into account variance in data availability and typological diversity. We select four high-resource source languages (English, Chinese, Japanese, Arabic) in order to go beyond English-centric transfer. We evaluate the cross-lingual transfer performance of these 4 languages to the 17 diverse languages. For DP, we chose the subset that occurs in UD. We highlight the properties of all 21 languages in Table 1. 4.1 Baselines mBERT (Standard Transfer Setup). We primarily focus on mBERT as it has been shown to work well for low-resource languages (Pfeiffer et al., 2020b). mBERT is trained on the 104 languages with the largest Wikipedias.6In the standard crosslingual transfer setting (see 2), the full model is ne-tuned on the target task in the (high-resource) source language, and is evaluated on the test set of the target (low-resource) language. MAD-X. '}),\n",
       " 'prompt_tmpl2': ComponentIntermediates(inputs={'input': 'What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?', 'context_str': '2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly lower computational cost.Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions regarding how well the cross-lingual mBERT representation captures lexical semantics, motivating our work. In this paper, we directly assess the cross-lingual properties of multilingual representations on tasks where lexical semantics plays an important role and present one unsuccessful and two successful methods for achieving better language neutrality. Multilingual capabilities of representations are often evaluated by zero-shot transfer from the training language to a test language (Hu et al., 2020; Liang et al., 2020). However, in such a setup, we can never be sure if the probing model did not overt for the original language, as training is usually stopped when accuracy decreases on a validation set from the same language (otherwise, it would not be zero-shot), even when it would have been better to stop the training earlier. This overtting on the original language can pose a disadvantage for information-richer representations. To avoid such methodological issues, we select tasks that only involve a direct comparison of the representations with no training: cross-lingual sentence retrieval, word alignment (WA), and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language ID classiers and assessing how the representation similarity corresponds to phylogenetic language families. We nd that contextual representations are more language-neutral than static word embeddings which have been explicitly trained to represent matching words similarly and can be used in a\\n\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, November 37, 2019. c2019 Association for Computational Linguistics833Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT Shijie Wu and Mark Dredze Department of Computer Science Johns Hopkins University shijie.wu@jhu.edu, mdredze@cs.jhu.edu Abstract Pretrained contextual representation models (Peters et al. ,2018 ;Devlin et al. ,2019 ) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT ( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classication, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and nd mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specic features, and measure factors that inuence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks ( Peters et al. ,2018 ;Howard and Ruder ,2018 ;Radford et al. \\n\\nLanguages are sorted by mBERT zero-shot transfer performance. Three downward triangles indicate performance drop more than the legends lower limit. 96.396.496.596.696.796.896.9 0 2 4 6 8 10 12 LayerAccuracy Figure 2: Language identication accuracy for different layer of mBERT. layer 0 is the embedding layer and the layeri >0is output of the ithtransformer block. across languages in any task, we assume Ven trainis the set of all subwords in the English training set, V testis the set of all subwords in language test set, andc wis the count of subword win test set of language . We then calculate the percentage of observed subwords at type-level p typeand token-levelp token for each target language . p type=|V obs| |Vtest|100 p token= wV obsc w wV testcw100 whereV obs=Ven trainV test. In Fig. 3, we show the relation between crosslingual zero-shot transfer performance of mBERT andp typeorp token for all ve tasks with Pearson correlation. In four out of ve tasks (not XNLI) we observed a strong positive correlation ( p <0.05) with a correlation coefcient larger than 0.5. In Indo-European languages, we observed p token is usually around 50% to 75% while p typeis usually less than 50%. This indicates that subwords shared across languages are usually high frequency6. We 6With the data-dependent WordPiece algorithm, subwords that appear in multiple languages with high frequency are more likely to be selected.\\n\\nFor XLM-R, Conneau et al. (2020) observe that for a xed model capacity, downstream cross-lingual transfer improves with more pretraining languages up to a point after which adding more pretraining languages hurts downstream transfer. This effect, termed the curse of multilinguality, can be mitigated by increasing models capacity (Artetxe et al., 2020b) or additional training for particular language pairs (Pfeiffer et al., 2020). This points to MMTs capacity (i.e., computational budgets), as a critical factor for effective zero-shot transfer. In contrast, we identify few-shot transfer as a much more cost-effective strategy for improving downstream target language performance ( 4). We show for a number of target languages and downstream tasks, that one can obtain large performance gains at very small annotation cost, without having to pretrain from scratch an MMT of larger capacity. 2.3 Cross-Lingual Transfer with MMTs A body of recent work probed the knowledge encoded in MMTs, primarily mBERT. Libovick `y et al. (2020) analyze language-specic versus languageuniversal knowledge encoded in mBERT. Pires et al. (2019) demonstrate mBERT to be effective for POS-tagging and NER zero-shot transfer between related languages. Wu and Dredze (2019) extend this analysis to more tasks and languages, and show that mBERT-based transfer is on a par with the best task-specic zero-shot transfer approaches. Similarly, Karthikeyan et al. (2020) prove mBERT to be effective for NER and NLI transfer to Hindi, Spanish, and Russian.1Importantly, they show that transfer effectiveness does not depend on the vocabulary overlap between the languages. In most recent work, concurrent to this, Hu et al. (2020) introduce XTREME, a benchmark for eval1Note that all three are high-resource Indo-European languages with large Wikipedias.uating multilingual encoders encompassing 9 tasks and 40 languages.2While the primary focus is a large-scale zero-shot transfer evaluation, they also experiment with target-language ne-tuning (1,000 instances for POS and NER). While Hu et al. (2020) focus on the evaluation aspects and protocols, in this work, we provide a more detailed analysis of the factors that hinder effective zeroshot transfer across several tasks.3We also put more emphasis on few-shot transfer, and approach it differently: by sequentially ne-tuning MMTs, rst on (larger) source language training data and then on few target-language instances. Artetxe et al. \\n\\n10191Languages. WikiAnn offers a wide language coverage (176 languages) and, consequently, a number of language-related comparative analyses. In order to systematically compare against state-of-the-art cross-lingual methods under different evaluation conditions, we identify 1)low-resource languages where the script has been covered by mBERT but the model has not been specically trained on the language 2)as well as low-resource languages with scripts not covered at all by pretraining. In each case, we select four languages, taking into account variance in data availability and typological diversity. We select four high-resource source languages (English, Chinese, Japanese, Arabic) in order to go beyond English-centric transfer. We evaluate the cross-lingual transfer performance of these 4 languages to the 17 diverse languages. For DP, we chose the subset that occurs in UD. We highlight the properties of all 21 languages in Table 1. 4.1 Baselines mBERT (Standard Transfer Setup). We primarily focus on mBERT as it has been shown to work well for low-resource languages (Pfeiffer et al., 2020b). mBERT is trained on the 104 languages with the largest Wikipedias.6In the standard crosslingual transfer setting (see 2), the full model is ne-tuned on the target task in the (high-resource) source language, and is evaluated on the test set of the target (low-resource) language. MAD-X. '}, outputs={'prompt': \"\\n    <|begin_of_text|>\\n    <|start_header_id|>\\n        system\\n    <|end_header_id|>\\n        You are an AI assistant tasked with answering questions based on provided context.\\n        Your goal is to provide accurate, relevant, and concise responses using only the information given in the context.\\n        If the context doesn't contain enough information to answer the question fully, state that clearly. \\n        Do not make up or infer information beyond what's explicitly stated in the context.\\n\\n        [INSTRUCTIONS]\\n        1. Carefully read the provided context and query.\\n        2. Analyze the information in the context that is relevant to the query.\\n        3. Formulate a clear and concise answer based solely on the given context.\\n        4. If the context doesn't provide sufficient information to answer the query, state this explicitly.\\n        5. Do not include any information that is not present in the given context.\\n    <|eot_id|>\\n    <|start_header_id|>\\n        user\\n    <|end_header_id|>\\n    Answer the user question based on the context provided below\\n\\n    [CONTEXT]:\\n    2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly lower computational cost.Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions regarding how well the cross-lingual mBERT representation captures lexical semantics, motivating our work. In this paper, we directly assess the cross-lingual properties of multilingual representations on tasks where lexical semantics plays an important role and present one unsuccessful and two successful methods for achieving better language neutrality. Multilingual capabilities of representations are often evaluated by zero-shot transfer from the training language to a test language (Hu et al., 2020; Liang et al., 2020). However, in such a setup, we can never be sure if the probing model did not overt for the original language, as training is usually stopped when accuracy decreases on a validation set from the same language (otherwise, it would not be zero-shot), even when it would have been better to stop the training earlier. This overtting on the original language can pose a disadvantage for information-richer representations. To avoid such methodological issues, we select tasks that only involve a direct comparison of the representations with no training: cross-lingual sentence retrieval, word alignment (WA), and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language ID classiers and assessing how the representation similarity corresponds to phylogenetic language families. We nd that contextual representations are more language-neutral than static word embeddings which have been explicitly trained to represent matching words similarly and can be used in a\\n\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, November 37, 2019. c2019 Association for Computational Linguistics833Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT Shijie Wu and Mark Dredze Department of Computer Science Johns Hopkins University shijie.wu@jhu.edu, mdredze@cs.jhu.edu Abstract Pretrained contextual representation models (Peters et al. ,2018 ;Devlin et al. ,2019 ) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT ( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classication, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and nd mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specic features, and measure factors that inuence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks ( Peters et al. ,2018 ;Howard and Ruder ,2018 ;Radford et al. \\n\\nLanguages are sorted by mBERT zero-shot transfer performance. Three downward triangles indicate performance drop more than the legends lower limit. 96.396.496.596.696.796.896.9 0 2 4 6 8 10 12 LayerAccuracy Figure 2: Language identication accuracy for different layer of mBERT. layer 0 is the embedding layer and the layeri >0is output of the ithtransformer block. across languages in any task, we assume Ven trainis the set of all subwords in the English training set, V testis the set of all subwords in language test set, andc wis the count of subword win test set of language . We then calculate the percentage of observed subwords at type-level p typeand token-levelp token for each target language . p type=|V obs| |Vtest|100 p token= wV obsc w wV testcw100 whereV obs=Ven trainV test. In Fig. 3, we show the relation between crosslingual zero-shot transfer performance of mBERT andp typeorp token for all ve tasks with Pearson correlation. In four out of ve tasks (not XNLI) we observed a strong positive correlation ( p <0.05) with a correlation coefcient larger than 0.5. In Indo-European languages, we observed p token is usually around 50% to 75% while p typeis usually less than 50%. This indicates that subwords shared across languages are usually high frequency6. We 6With the data-dependent WordPiece algorithm, subwords that appear in multiple languages with high frequency are more likely to be selected.\\n\\nFor XLM-R, Conneau et al. (2020) observe that for a xed model capacity, downstream cross-lingual transfer improves with more pretraining languages up to a point after which adding more pretraining languages hurts downstream transfer. This effect, termed the curse of multilinguality, can be mitigated by increasing models capacity (Artetxe et al., 2020b) or additional training for particular language pairs (Pfeiffer et al., 2020). This points to MMTs capacity (i.e., computational budgets), as a critical factor for effective zero-shot transfer. In contrast, we identify few-shot transfer as a much more cost-effective strategy for improving downstream target language performance ( 4). We show for a number of target languages and downstream tasks, that one can obtain large performance gains at very small annotation cost, without having to pretrain from scratch an MMT of larger capacity. 2.3 Cross-Lingual Transfer with MMTs A body of recent work probed the knowledge encoded in MMTs, primarily mBERT. Libovick `y et al. (2020) analyze language-specic versus languageuniversal knowledge encoded in mBERT. Pires et al. (2019) demonstrate mBERT to be effective for POS-tagging and NER zero-shot transfer between related languages. Wu and Dredze (2019) extend this analysis to more tasks and languages, and show that mBERT-based transfer is on a par with the best task-specic zero-shot transfer approaches. Similarly, Karthikeyan et al. (2020) prove mBERT to be effective for NER and NLI transfer to Hindi, Spanish, and Russian.1Importantly, they show that transfer effectiveness does not depend on the vocabulary overlap between the languages. In most recent work, concurrent to this, Hu et al. (2020) introduce XTREME, a benchmark for eval1Note that all three are high-resource Indo-European languages with large Wikipedias.uating multilingual encoders encompassing 9 tasks and 40 languages.2While the primary focus is a large-scale zero-shot transfer evaluation, they also experiment with target-language ne-tuning (1,000 instances for POS and NER). While Hu et al. (2020) focus on the evaluation aspects and protocols, in this work, we provide a more detailed analysis of the factors that hinder effective zeroshot transfer across several tasks.3We also put more emphasis on few-shot transfer, and approach it differently: by sequentially ne-tuning MMTs, rst on (larger) source language training data and then on few target-language instances. Artetxe et al. \\n\\n10191Languages. WikiAnn offers a wide language coverage (176 languages) and, consequently, a number of language-related comparative analyses. In order to systematically compare against state-of-the-art cross-lingual methods under different evaluation conditions, we identify 1)low-resource languages where the script has been covered by mBERT but the model has not been specically trained on the language 2)as well as low-resource languages with scripts not covered at all by pretraining. In each case, we select four languages, taking into account variance in data availability and typological diversity. We select four high-resource source languages (English, Chinese, Japanese, Arabic) in order to go beyond English-centric transfer. We evaluate the cross-lingual transfer performance of these 4 languages to the 17 diverse languages. For DP, we chose the subset that occurs in UD. We highlight the properties of all 21 languages in Table 1. 4.1 Baselines mBERT (Standard Transfer Setup). We primarily focus on mBERT as it has been shown to work well for low-resource languages (Pfeiffer et al., 2020b). mBERT is trained on the 104 languages with the largest Wikipedias.6In the standard crosslingual transfer setting (see 2), the full model is ne-tuned on the target task in the (high-resource) source language, and is evaluated on the test set of the target (low-resource) language. MAD-X. \\n    \\n    [QUERY]\\n    What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?\\n    \\n    <|eot_id|>\\n    <|start_header_id|>\\n        assistant\\n    <|end_header_id|>\\n    \"}),\n",
       " 'llm': ComponentIntermediates(inputs={'messages': \"\\n    <|begin_of_text|>\\n    <|start_header_id|>\\n        system\\n    <|end_header_id|>\\n        You are an AI assistant tasked with answering questions based on provided context.\\n        Your goal is to provide accurate, relevant, and concise responses using only the information given in the context.\\n        If the context doesn't contain enough information to answer the question fully, state that clearly. \\n        Do not make up or infer information beyond what's explicitly stated in the context.\\n\\n        [INSTRUCTIONS]\\n        1. Carefully read the provided context and query.\\n        2. Analyze the information in the context that is relevant to the query.\\n        3. Formulate a clear and concise answer based solely on the given context.\\n        4. If the context doesn't provide sufficient information to answer the query, state this explicitly.\\n        5. Do not include any information that is not present in the given context.\\n    <|eot_id|>\\n    <|start_header_id|>\\n        user\\n    <|end_header_id|>\\n    Answer the user question based on the context provided below\\n\\n    [CONTEXT]:\\n    2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly lower computational cost.Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions regarding how well the cross-lingual mBERT representation captures lexical semantics, motivating our work. In this paper, we directly assess the cross-lingual properties of multilingual representations on tasks where lexical semantics plays an important role and present one unsuccessful and two successful methods for achieving better language neutrality. Multilingual capabilities of representations are often evaluated by zero-shot transfer from the training language to a test language (Hu et al., 2020; Liang et al., 2020). However, in such a setup, we can never be sure if the probing model did not overt for the original language, as training is usually stopped when accuracy decreases on a validation set from the same language (otherwise, it would not be zero-shot), even when it would have been better to stop the training earlier. This overtting on the original language can pose a disadvantage for information-richer representations. To avoid such methodological issues, we select tasks that only involve a direct comparison of the representations with no training: cross-lingual sentence retrieval, word alignment (WA), and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language ID classiers and assessing how the representation similarity corresponds to phylogenetic language families. We nd that contextual representations are more language-neutral than static word embeddings which have been explicitly trained to represent matching words similarly and can be used in a\\n\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, November 37, 2019. c2019 Association for Computational Linguistics833Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT Shijie Wu and Mark Dredze Department of Computer Science Johns Hopkins University shijie.wu@jhu.edu, mdredze@cs.jhu.edu Abstract Pretrained contextual representation models (Peters et al. ,2018 ;Devlin et al. ,2019 ) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT ( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classication, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and nd mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specic features, and measure factors that inuence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks ( Peters et al. ,2018 ;Howard and Ruder ,2018 ;Radford et al. \\n\\nLanguages are sorted by mBERT zero-shot transfer performance. Three downward triangles indicate performance drop more than the legends lower limit. 96.396.496.596.696.796.896.9 0 2 4 6 8 10 12 LayerAccuracy Figure 2: Language identication accuracy for different layer of mBERT. layer 0 is the embedding layer and the layeri >0is output of the ithtransformer block. across languages in any task, we assume Ven trainis the set of all subwords in the English training set, V testis the set of all subwords in language test set, andc wis the count of subword win test set of language . We then calculate the percentage of observed subwords at type-level p typeand token-levelp token for each target language . p type=|V obs| |Vtest|100 p token= wV obsc w wV testcw100 whereV obs=Ven trainV test. In Fig. 3, we show the relation between crosslingual zero-shot transfer performance of mBERT andp typeorp token for all ve tasks with Pearson correlation. In four out of ve tasks (not XNLI) we observed a strong positive correlation ( p <0.05) with a correlation coefcient larger than 0.5. In Indo-European languages, we observed p token is usually around 50% to 75% while p typeis usually less than 50%. This indicates that subwords shared across languages are usually high frequency6. We 6With the data-dependent WordPiece algorithm, subwords that appear in multiple languages with high frequency are more likely to be selected.\\n\\nFor XLM-R, Conneau et al. (2020) observe that for a xed model capacity, downstream cross-lingual transfer improves with more pretraining languages up to a point after which adding more pretraining languages hurts downstream transfer. This effect, termed the curse of multilinguality, can be mitigated by increasing models capacity (Artetxe et al., 2020b) or additional training for particular language pairs (Pfeiffer et al., 2020). This points to MMTs capacity (i.e., computational budgets), as a critical factor for effective zero-shot transfer. In contrast, we identify few-shot transfer as a much more cost-effective strategy for improving downstream target language performance ( 4). We show for a number of target languages and downstream tasks, that one can obtain large performance gains at very small annotation cost, without having to pretrain from scratch an MMT of larger capacity. 2.3 Cross-Lingual Transfer with MMTs A body of recent work probed the knowledge encoded in MMTs, primarily mBERT. Libovick `y et al. (2020) analyze language-specic versus languageuniversal knowledge encoded in mBERT. Pires et al. (2019) demonstrate mBERT to be effective for POS-tagging and NER zero-shot transfer between related languages. Wu and Dredze (2019) extend this analysis to more tasks and languages, and show that mBERT-based transfer is on a par with the best task-specic zero-shot transfer approaches. Similarly, Karthikeyan et al. (2020) prove mBERT to be effective for NER and NLI transfer to Hindi, Spanish, and Russian.1Importantly, they show that transfer effectiveness does not depend on the vocabulary overlap between the languages. In most recent work, concurrent to this, Hu et al. (2020) introduce XTREME, a benchmark for eval1Note that all three are high-resource Indo-European languages with large Wikipedias.uating multilingual encoders encompassing 9 tasks and 40 languages.2While the primary focus is a large-scale zero-shot transfer evaluation, they also experiment with target-language ne-tuning (1,000 instances for POS and NER). While Hu et al. (2020) focus on the evaluation aspects and protocols, in this work, we provide a more detailed analysis of the factors that hinder effective zeroshot transfer across several tasks.3We also put more emphasis on few-shot transfer, and approach it differently: by sequentially ne-tuning MMTs, rst on (larger) source language training data and then on few target-language instances. Artetxe et al. \\n\\n10191Languages. WikiAnn offers a wide language coverage (176 languages) and, consequently, a number of language-related comparative analyses. In order to systematically compare against state-of-the-art cross-lingual methods under different evaluation conditions, we identify 1)low-resource languages where the script has been covered by mBERT but the model has not been specically trained on the language 2)as well as low-resource languages with scripts not covered at all by pretraining. In each case, we select four languages, taking into account variance in data availability and typological diversity. We select four high-resource source languages (English, Chinese, Japanese, Arabic) in order to go beyond English-centric transfer. We evaluate the cross-lingual transfer performance of these 4 languages to the 17 diverse languages. For DP, we chose the subset that occurs in UD. We highlight the properties of all 21 languages in Table 1. 4.1 Baselines mBERT (Standard Transfer Setup). We primarily focus on mBERT as it has been shown to work well for low-resource languages (Pfeiffer et al., 2020b). mBERT is trained on the 104 languages with the largest Wikipedias.6In the standard crosslingual transfer setting (see 2), the full model is ne-tuned on the target task in the (high-resource) source language, and is evaluated on the test set of the target (low-resource) language. MAD-X. \\n    \\n    [QUERY]\\n    What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?\\n    \\n    <|eot_id|>\\n    <|start_header_id|>\\n        assistant\\n    <|end_header_id|>\\n    \"}, outputs={'output': ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='According to the context, one limitation of using mBERT for zero-shot cross-lingual transfer is that it can be hindered by the \"curse of multilinguality\", where adding more pretraining languages up to a point hurts downstream transfer. This effect can be mitigated by increasing model capacity or additional training for particular language pairs.\\n\\nAdditionally, when new scripts or languages are introduced, mBERT may not have been specifically trained on these languages, which can lead to decreased performance. The text mentions that mBERT is pre-trained on the 104 languages with the largest Wikipedias, implying that languages without a large Wikipedia presence may not be well-represented in the model.\\n\\nThere is also a mention of the \"overt-fitting\" issue when training models for zero-shot transfer, where the model overfits to the original language rather than generalizing to new languages. This can make it difficult to ensure that the probing model did not overfit to the original language.\\n\\nThe text does not specify how these limitations affect performance specifically, but it implies that there are challenges associated with using mBERT for zero-shot cross-lingual transfer when working with new scripts or languages.', additional_kwargs={'tool_calls': []}), raw={'model': 'llama3.1:latest', 'created_at': '2024-08-25T19:03:54.707853Z', 'message': {'role': 'assistant', 'content': 'According to the context, one limitation of using mBERT for zero-shot cross-lingual transfer is that it can be hindered by the \"curse of multilinguality\", where adding more pretraining languages up to a point hurts downstream transfer. This effect can be mitigated by increasing model capacity or additional training for particular language pairs.\\n\\nAdditionally, when new scripts or languages are introduced, mBERT may not have been specifically trained on these languages, which can lead to decreased performance. The text mentions that mBERT is pre-trained on the 104 languages with the largest Wikipedias, implying that languages without a large Wikipedia presence may not be well-represented in the model.\\n\\nThere is also a mention of the \"overt-fitting\" issue when training models for zero-shot transfer, where the model overfits to the original language rather than generalizing to new languages. This can make it difficult to ensure that the probing model did not overfit to the original language.\\n\\nThe text does not specify how these limitations affect performance specifically, but it implies that there are challenges associated with using mBERT for zero-shot cross-lingual transfer when working with new scripts or languages.'}, 'done_reason': 'stop', 'done': True, 'total_duration': 35070166458, 'load_duration': 14294382375, 'prompt_eval_count': 2239, 'prompt_eval_duration': 10185746000, 'eval_count': 237, 'eval_duration': 10587653000}, delta=None, logprobs=None, additional_kwargs={})})}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}\n",
      "{'page_label': '1', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}\n",
      "{'page_label': '8', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}\n",
      "{'page_label': '3', 'file_name': 'lauscher2020_fromzero_to_hero.pdf', 'file_path': 'data/pdf_documents/lauscher2020_fromzero_to_hero.pdf', 'file_type': 'application/pdf', 'file_size': 4938400, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}\n",
      "{'page_label': '6', 'file_name': 'pfeiffer2021_adapting_multilingual_lms_to_new_scripts_unks.pdf', 'file_path': 'data/pdf_documents/pfeiffer2021_adapting_multilingual_lms_to_new_scripts_unks.pdf', 'file_type': 'application/pdf', 'file_size': 661550, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}\n"
     ]
    }
   ],
   "source": [
    "for i in intermediates[\"retriever\"].outputs[\"output\"]:\n",
    "    print(i.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, one limitation of using mBERT for zero-shot cross-lingual transfer is that it can be hindered by the \"curse of multilinguality\", where adding more pretraining languages up to a point hurts downstream transfer. This effect can be mitigated by increasing model capacity or additional training for particular language pairs.\n",
      "\n",
      "Additionally, when new scripts or languages are introduced, mBERT may not have been specifically trained on these languages, which can lead to decreased performance. The text mentions that mBERT is pre-trained on the 104 languages with the largest Wikipedias, implying that languages without a large Wikipedia presence may not be well-represented in the model.\n",
      "\n",
      "There is also a mention of the \"overt-fitting\" issue when training models for zero-shot transfer, where the model overfits to the original language rather than generalizing to new languages. This can make it difficult to ensure that the probing model did not overfit to the original language.\n",
      "\n",
      "The text does not specify how these limitations affect performance specifically, but it implies that there are challenges associated with using mBERT for zero-shot cross-lingual transfer when working with new scripts or languages.\n"
     ]
    }
   ],
   "source": [
    "print(output.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusing Retrieval with Semantic Chunking and Query Transformation\n",
    "\n",
    "This code implements a Fusion Retrieval system that combines vector-based similarity search with keyword-based BM25 retrieval. The approach aims to leverage the strengths of both methods to improve the overall quality and relevance of document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "doc_store_name = \"semantic-document-nodes\"\n",
    "nodes_ = get_doc_store(doc_store_name)\n",
    "\n",
    "\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=nodes_, similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "from typing import List\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from types import MethodType \n",
    "\n",
    "def get_queries(self, original_query: str) -> List[QueryBundle]:\n",
    "    prompt_str1 = \"Represent this sentence for searching relevant passages: {input}\"\n",
    "    prompt_str = self.query_gen_prompt.format(\n",
    "        num_queries=self.num_queries - 1,\n",
    "        query=original_query,\n",
    "    )\n",
    "    response = self._llm.complete(prompt_str)\n",
    "\n",
    "    # assume LLM proper put each query on a newline\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    queries = [prompt_str1.format(input=q.strip()) for q in queries if q.strip()]\n",
    "    if self._verbose:\n",
    "        queries_str = \"\\n\".join(queries)\n",
    "        print(f\"Generated queries:\\n{queries_str}\")\n",
    "\n",
    "    # The LLM often returns more queries than we asked for, so trim the list.\n",
    "    return [QueryBundle(q) for q in queries[: self.num_queries - 1]]\n",
    "\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=4,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\n",
    ")\n",
    "\n",
    "retriever._get_queries = MethodType(get_queries, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply nested async to run in a notebook\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate question regarding topic\n",
    "prompt_str1 = \"{topic}\"\n",
    "prompt_tmpl1 = PromptTemplate(prompt_str1)\n",
    "\n",
    "\n",
    "query_pipeline = QueryPipeline(\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "query_pipeline.add_modules(\n",
    "    {\n",
    "        \"input\":InputComponent(),\n",
    "        \"prompt_tmpl1\":prompt_tmpl1,\n",
    "        \"retriever\":retriever,\n",
    "        \"concat_component\": concat_component,\n",
    "        \"prompt_tmpl2\":prompt_tmpl2,\n",
    "        \"llm\":Settings.llm\n",
    "    }\n",
    ")\n",
    "\n",
    "query_pipeline.add_link(\"input\",\"prompt_tmpl1\")\n",
    "query_pipeline.add_link(\"prompt_tmpl1\",\"retriever\")\n",
    "query_pipeline.add_link(\"retriever\",\"concat_component\")\n",
    "query_pipeline.add_link(\"input\",\"prompt_tmpl2\",dest_key=\"input\")\n",
    "query_pipeline.add_link(\"concat_component\",\"prompt_tmpl2\",dest_key=\"context_str\")\n",
    "query_pipeline.add_link(\"prompt_tmpl2\",\"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "input: What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module prompt_tmpl1 with input: \n",
      "topic: What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module retriever with input: \n",
      "input: What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?\n",
      "\n",
      "\u001b[0mGenerated queries:\n",
      "Represent this sentence for searching relevant passages: Here are three search queries related to the input query:\n",
      "Represent this sentence for searching relevant passages: 1. \"mBERT limitations in zero-shot cross-lingual NLP models\"\n",
      "Represent this sentence for searching relevant passages: 2. \"Impact of script variation on mBERT performance for low-resource languages\"\n",
      "Represent this sentence for searching relevant passages: 3. \"Evaluation of mBERT's adaptability to new languages and writing systems in zero-shot cross-lingual tasks\"\n",
      "\u001b[1;3;38;2;155;135;227m> Running module concat_component with input: \n",
      "retrieved_nodes: [NodeWithScore(node=TextNode(id_='3a121c11-fa9c-495b-85f8-e79b55fad7a5', embedding=None, metadata={'page_label': '1', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/w...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module prompt_tmpl2 with input: \n",
      "input: What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?\n",
      "context_str: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, Novem...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
      "messages: \n",
      "    <|begin_of_text|>\n",
      "    <|start_header_id|>\n",
      "        system\n",
      "    <|end_header_id|>\n",
      "        You are an AI assistant tasked with answering questions based on provided context.\n",
      "        Your goal is to p...\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Based on the provided context, there is no specific information that lists the limitations of using mBERT for zero-shot cross-lingual tasks. However, the paper mentions that mBERT does not outperform other models (specifically, DistilBERT) on certain tasks (XNLI and NER), which could be considered a limitation.\\n\\nRegarding new scripts or languages being introduced, there is no explicit information in the provided context to discuss how these might affect the limitations of using mBERT for zero-shot cross-lingual tasks.', additional_kwargs={'tool_calls': []}), raw={'model': 'llama3.1:latest', 'created_at': '2024-08-25T19:13:05.243769Z', 'message': {'role': 'assistant', 'content': 'Based on the provided context, there is no specific information that lists the limitations of using mBERT for zero-shot cross-lingual tasks. However, the paper mentions that mBERT does not outperform other models (specifically, DistilBERT) on certain tasks (XNLI and NER), which could be considered a limitation.\\n\\nRegarding new scripts or languages being introduced, there is no explicit information in the provided context to discuss how these might affect the limitations of using mBERT for zero-shot cross-lingual tasks.'}, 'done_reason': 'stop', 'done': True, 'total_duration': 9328346334, 'load_duration': 54800667, 'prompt_eval_count': 1043, 'prompt_eval_duration': 5131785000, 'eval_count': 107, 'eval_duration': 4135847000}, delta=None, logprobs=None, additional_kwargs={})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# response_1 = query_pipeline.run(topic=query)\n",
    "output, intermediates = query_pipeline.run_with_intermediates(input=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, there is no specific information that lists the limitations of using mBERT for zero-shot cross-lingual tasks. However, the paper mentions that mBERT does not outperform other models (specifically, DistilBERT) on certain tasks (XNLI and NER), which could be considered a limitation.\n",
      "\n",
      "Regarding new scripts or languages being introduced, there is no explicit information in the provided context to discuss how these might affect the limitations of using mBERT for zero-shot cross-lingual tasks.\n"
     ]
    }
   ],
   "source": [
    "print(output.message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': ComponentIntermediates(inputs={'input': 'What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?'}, outputs={'input': 'What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?'}),\n",
       " 'prompt_tmpl1': ComponentIntermediates(inputs={'topic': 'What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?'}, outputs={'prompt': 'What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?'}),\n",
       " 'retriever': ComponentIntermediates(inputs={'input': 'What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?'}, outputs={'output': [NodeWithScore(node=TextNode(id_='3a121c11-fa9c-495b-85f8-e79b55fad7a5', embedding=None, metadata={'page_label': '1', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='762a1ca6-7729-440c-86c8-a1ffc0242596', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='2e40bf108e042f9985f3c7f6b071c59890907d489f20a12448490e92003477f2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2c695a8d-1d05-4d7b-9b7d-c26bff07ea16', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='bef3f6d88b213b02ee2ec374796b8fd749a752a8731338fdaa7f3d750bb4c6db')}, text='Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, November 37, 2019. c2019 Association for Computational Linguistics833Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT Shijie Wu and Mark Dredze Department of Computer Science Johns Hopkins University shijie.wu@jhu.edu, mdredze@cs.jhu.edu Abstract Pretrained contextual representation models (Peters et al. ,2018 ;Devlin et al. ,2019 ) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT ( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classication, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and nd mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specic features, and measure factors that inuence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks ( Peters et al. ,2018 ;Howard and Ruder ,2018 ;Radford et al. ', mimetype='text/plain', start_char_idx=0, end_char_idx=1629, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.05), NodeWithScore(node=TextNode(id_='6f27def0-4877-4535-aac5-001f41acc5d1', embedding=None, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='490cf48f-97c9-4215-a71e-187753c9eeb1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='12e77fb0ab491f39fd6abb121c270e3887a2a0270e90cbb9a301f09618bb2626'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='50367415-5691-4eb4-aca3-577b65f7a4e8', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='5c519a54684e934fd33ecf31e2d6154ba8ef87667abd8932588e65bb3e35a7d5')}, text='2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly lower computational cost.Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions regarding how well the cross-lingual mBERT representation captures lexical semantics, motivating our work. In this paper, we directly assess the cross-lingual properties of multilingual representations on tasks where lexical semantics plays an important role and present one unsuccessful and two successful methods for achieving better language neutrality. Multilingual capabilities of representations are often evaluated by zero-shot transfer from the training language to a test language (Hu et al., 2020; Liang et al., 2020). However, in such a setup, we can never be sure if the probing model did not overt for the original language, as training is usually stopped when accuracy decreases on a validation set from the same language (otherwise, it would not be zero-shot), even when it would have been better to stop the training earlier. This overtting on the original language can pose a disadvantage for information-richer representations. To avoid such methodological issues, we select tasks that only involve a direct comparison of the representations with no training: cross-lingual sentence retrieval, word alignment (WA), and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language ID classiers and assessing how the representation similarity corresponds to phylogenetic language families. We nd that contextual representations are more language-neutral than static word embeddings which have been explicitly trained to represent matching words similarly and can be used in a', mimetype='text/plain', start_char_idx=2037, end_char_idx=4172, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.04918032786885246)]}),\n",
       " 'concat_component': ComponentIntermediates(inputs={'retrieved_nodes': [NodeWithScore(node=TextNode(id_='3a121c11-fa9c-495b-85f8-e79b55fad7a5', embedding=None, metadata={'page_label': '1', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='762a1ca6-7729-440c-86c8-a1ffc0242596', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'wudredze2019_betobentzbecas.pdf', 'file_path': 'data/pdf_documents/wudredze2019_betobentzbecas.pdf', 'file_type': 'application/pdf', 'file_size': 400689, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='2e40bf108e042f9985f3c7f6b071c59890907d489f20a12448490e92003477f2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2c695a8d-1d05-4d7b-9b7d-c26bff07ea16', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='bef3f6d88b213b02ee2ec374796b8fd749a752a8731338fdaa7f3d750bb4c6db')}, text='Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, November 37, 2019. c2019 Association for Computational Linguistics833Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT Shijie Wu and Mark Dredze Department of Computer Science Johns Hopkins University shijie.wu@jhu.edu, mdredze@cs.jhu.edu Abstract Pretrained contextual representation models (Peters et al. ,2018 ;Devlin et al. ,2019 ) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT ( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classication, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and nd mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specic features, and measure factors that inuence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks ( Peters et al. ,2018 ;Howard and Ruder ,2018 ;Radford et al. ', mimetype='text/plain', start_char_idx=0, end_char_idx=1629, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.05), NodeWithScore(node=TextNode(id_='6f27def0-4877-4535-aac5-001f41acc5d1', embedding=None, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='490cf48f-97c9-4215-a71e-187753c9eeb1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='12e77fb0ab491f39fd6abb121c270e3887a2a0270e90cbb9a301f09618bb2626'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='50367415-5691-4eb4-aca3-577b65f7a4e8', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'libov2020_how_neutral_is_mbert.pdf', 'file_path': 'data/pdf_documents/libov2020_how_neutral_is_mbert.pdf', 'file_type': 'application/pdf', 'file_size': 305799, 'creation_date': '2024-08-24', 'last_modified_date': '2024-08-01'}, hash='5c519a54684e934fd33ecf31e2d6154ba8ef87667abd8932588e65bb3e35a7d5')}, text='2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly lower computational cost.Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions regarding how well the cross-lingual mBERT representation captures lexical semantics, motivating our work. In this paper, we directly assess the cross-lingual properties of multilingual representations on tasks where lexical semantics plays an important role and present one unsuccessful and two successful methods for achieving better language neutrality. Multilingual capabilities of representations are often evaluated by zero-shot transfer from the training language to a test language (Hu et al., 2020; Liang et al., 2020). However, in such a setup, we can never be sure if the probing model did not overt for the original language, as training is usually stopped when accuracy decreases on a validation set from the same language (otherwise, it would not be zero-shot), even when it would have been better to stop the training earlier. This overtting on the original language can pose a disadvantage for information-richer representations. To avoid such methodological issues, we select tasks that only involve a direct comparison of the representations with no training: cross-lingual sentence retrieval, word alignment (WA), and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language ID classiers and assessing how the representation similarity corresponds to phylogenetic language families. We nd that contextual representations are more language-neutral than static word embeddings which have been explicitly trained to represent matching words similarly and can be used in a', mimetype='text/plain', start_char_idx=2037, end_char_idx=4172, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.04918032786885246)]}, outputs={'output': 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, November 37, 2019. c2019 Association for Computational Linguistics833Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT Shijie Wu and Mark Dredze Department of Computer Science Johns Hopkins University shijie.wu@jhu.edu, mdredze@cs.jhu.edu Abstract Pretrained contextual representation models (Peters et al. ,2018 ;Devlin et al. ,2019 ) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT ( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classication, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and nd mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specic features, and measure factors that inuence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks ( Peters et al. ,2018 ;Howard and Ruder ,2018 ;Radford et al. \\n\\n2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly lower computational cost.Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions regarding how well the cross-lingual mBERT representation captures lexical semantics, motivating our work. In this paper, we directly assess the cross-lingual properties of multilingual representations on tasks where lexical semantics plays an important role and present one unsuccessful and two successful methods for achieving better language neutrality. Multilingual capabilities of representations are often evaluated by zero-shot transfer from the training language to a test language (Hu et al., 2020; Liang et al., 2020). However, in such a setup, we can never be sure if the probing model did not overt for the original language, as training is usually stopped when accuracy decreases on a validation set from the same language (otherwise, it would not be zero-shot), even when it would have been better to stop the training earlier. This overtting on the original language can pose a disadvantage for information-richer representations. To avoid such methodological issues, we select tasks that only involve a direct comparison of the representations with no training: cross-lingual sentence retrieval, word alignment (WA), and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language ID classiers and assessing how the representation similarity corresponds to phylogenetic language families. We nd that contextual representations are more language-neutral than static word embeddings which have been explicitly trained to represent matching words similarly and can be used in a'}),\n",
       " 'prompt_tmpl2': ComponentIntermediates(inputs={'input': 'What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?', 'context_str': 'Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, November 37, 2019. c2019 Association for Computational Linguistics833Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT Shijie Wu and Mark Dredze Department of Computer Science Johns Hopkins University shijie.wu@jhu.edu, mdredze@cs.jhu.edu Abstract Pretrained contextual representation models (Peters et al. ,2018 ;Devlin et al. ,2019 ) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT ( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classication, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and nd mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specic features, and measure factors that inuence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks ( Peters et al. ,2018 ;Howard and Ruder ,2018 ;Radford et al. \\n\\n2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly lower computational cost.Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions regarding how well the cross-lingual mBERT representation captures lexical semantics, motivating our work. In this paper, we directly assess the cross-lingual properties of multilingual representations on tasks where lexical semantics plays an important role and present one unsuccessful and two successful methods for achieving better language neutrality. Multilingual capabilities of representations are often evaluated by zero-shot transfer from the training language to a test language (Hu et al., 2020; Liang et al., 2020). However, in such a setup, we can never be sure if the probing model did not overt for the original language, as training is usually stopped when accuracy decreases on a validation set from the same language (otherwise, it would not be zero-shot), even when it would have been better to stop the training earlier. This overtting on the original language can pose a disadvantage for information-richer representations. To avoid such methodological issues, we select tasks that only involve a direct comparison of the representations with no training: cross-lingual sentence retrieval, word alignment (WA), and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language ID classiers and assessing how the representation similarity corresponds to phylogenetic language families. We nd that contextual representations are more language-neutral than static word embeddings which have been explicitly trained to represent matching words similarly and can be used in a'}, outputs={'prompt': \"\\n    <|begin_of_text|>\\n    <|start_header_id|>\\n        system\\n    <|end_header_id|>\\n        You are an AI assistant tasked with answering questions based on provided context.\\n        Your goal is to provide accurate, relevant, and concise responses using only the information given in the context.\\n        If the context doesn't contain enough information to answer the question fully, state that clearly. \\n        Do not make up or infer information beyond what's explicitly stated in the context.\\n\\n        [INSTRUCTIONS]\\n        1. Carefully read the provided context and query.\\n        2. Analyze the information in the context that is relevant to the query.\\n        3. Formulate a clear and concise answer based solely on the given context.\\n        4. If the context doesn't provide sufficient information to answer the query, state this explicitly.\\n        5. Do not include any information that is not present in the given context.\\n    <|eot_id|>\\n    <|start_header_id|>\\n        user\\n    <|end_header_id|>\\n    Answer the user question based on the context provided below\\n\\n    [CONTEXT]:\\n    Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, November 37, 2019. c2019 Association for Computational Linguistics833Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT Shijie Wu and Mark Dredze Department of Computer Science Johns Hopkins University shijie.wu@jhu.edu, mdredze@cs.jhu.edu Abstract Pretrained contextual representation models (Peters et al. ,2018 ;Devlin et al. ,2019 ) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT ( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classication, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and nd mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specic features, and measure factors that inuence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks ( Peters et al. ,2018 ;Howard and Ruder ,2018 ;Radford et al. \\n\\n2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly lower computational cost.Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions regarding how well the cross-lingual mBERT representation captures lexical semantics, motivating our work. In this paper, we directly assess the cross-lingual properties of multilingual representations on tasks where lexical semantics plays an important role and present one unsuccessful and two successful methods for achieving better language neutrality. Multilingual capabilities of representations are often evaluated by zero-shot transfer from the training language to a test language (Hu et al., 2020; Liang et al., 2020). However, in such a setup, we can never be sure if the probing model did not overt for the original language, as training is usually stopped when accuracy decreases on a validation set from the same language (otherwise, it would not be zero-shot), even when it would have been better to stop the training earlier. This overtting on the original language can pose a disadvantage for information-richer representations. To avoid such methodological issues, we select tasks that only involve a direct comparison of the representations with no training: cross-lingual sentence retrieval, word alignment (WA), and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language ID classiers and assessing how the representation similarity corresponds to phylogenetic language families. We nd that contextual representations are more language-neutral than static word embeddings which have been explicitly trained to represent matching words similarly and can be used in a\\n    \\n    [QUERY]\\n    What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?\\n    \\n    <|eot_id|>\\n    <|start_header_id|>\\n        assistant\\n    <|end_header_id|>\\n    \"}),\n",
       " 'llm': ComponentIntermediates(inputs={'messages': \"\\n    <|begin_of_text|>\\n    <|start_header_id|>\\n        system\\n    <|end_header_id|>\\n        You are an AI assistant tasked with answering questions based on provided context.\\n        Your goal is to provide accurate, relevant, and concise responses using only the information given in the context.\\n        If the context doesn't contain enough information to answer the question fully, state that clearly. \\n        Do not make up or infer information beyond what's explicitly stated in the context.\\n\\n        [INSTRUCTIONS]\\n        1. Carefully read the provided context and query.\\n        2. Analyze the information in the context that is relevant to the query.\\n        3. Formulate a clear and concise answer based solely on the given context.\\n        4. If the context doesn't provide sufficient information to answer the query, state this explicitly.\\n        5. Do not include any information that is not present in the given context.\\n    <|eot_id|>\\n    <|start_header_id|>\\n        user\\n    <|end_header_id|>\\n    Answer the user question based on the context provided below\\n\\n    [CONTEXT]:\\n    Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 833844, Hong Kong, China, November 37, 2019. c2019 Association for Computational Linguistics833Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT Shijie Wu and Mark Dredze Department of Computer Science Johns Hopkins University shijie.wu@jhu.edu, mdredze@cs.jhu.edu Abstract Pretrained contextual representation models (Peters et al. ,2018 ;Devlin et al. ,2019 ) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT ( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classication, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and nd mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specic features, and measure factors that inuence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks ( Peters et al. ,2018 ;Howard and Ruder ,2018 ;Radford et al. \\n\\n2019), claimed to outperform mBERT both on XNLI and NER tasks. We also study DistilBERT (Sanh et al., 2019) applied to mBERT, which promises to deliver comparable results to mBERT at a signicantly lower computational cost.Pires et al. (2019) present an exploratory paper showing that mBERT can be used cross-lingually for zero-shot transfer in morphological and syntactic tasks, at least for typologically similar languages. They also study an interesting semantic task, sentence-retrieval, with promising initial results. Their work leaves many open questions regarding how well the cross-lingual mBERT representation captures lexical semantics, motivating our work. In this paper, we directly assess the cross-lingual properties of multilingual representations on tasks where lexical semantics plays an important role and present one unsuccessful and two successful methods for achieving better language neutrality. Multilingual capabilities of representations are often evaluated by zero-shot transfer from the training language to a test language (Hu et al., 2020; Liang et al., 2020). However, in such a setup, we can never be sure if the probing model did not overt for the original language, as training is usually stopped when accuracy decreases on a validation set from the same language (otherwise, it would not be zero-shot), even when it would have been better to stop the training earlier. This overtting on the original language can pose a disadvantage for information-richer representations. To avoid such methodological issues, we select tasks that only involve a direct comparison of the representations with no training: cross-lingual sentence retrieval, word alignment (WA), and machine translation quality estimation (MT QE). Additionally, we explore how the language is represented in the embeddings by training language ID classiers and assessing how the representation similarity corresponds to phylogenetic language families. We nd that contextual representations are more language-neutral than static word embeddings which have been explicitly trained to represent matching words similarly and can be used in a\\n    \\n    [QUERY]\\n    What are the limitations of using mBERT for zero-shot cross-lingual tasks, and how do these limitations vary when new scripts or languages are introduced?\\n    \\n    <|eot_id|>\\n    <|start_header_id|>\\n        assistant\\n    <|end_header_id|>\\n    \"}, outputs={'output': ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Based on the provided context, there is no specific information that lists the limitations of using mBERT for zero-shot cross-lingual tasks. However, the paper mentions that mBERT does not outperform other models (specifically, DistilBERT) on certain tasks (XNLI and NER), which could be considered a limitation.\\n\\nRegarding new scripts or languages being introduced, there is no explicit information in the provided context to discuss how these might affect the limitations of using mBERT for zero-shot cross-lingual tasks.', additional_kwargs={'tool_calls': []}), raw={'model': 'llama3.1:latest', 'created_at': '2024-08-25T19:13:05.243769Z', 'message': {'role': 'assistant', 'content': 'Based on the provided context, there is no specific information that lists the limitations of using mBERT for zero-shot cross-lingual tasks. However, the paper mentions that mBERT does not outperform other models (specifically, DistilBERT) on certain tasks (XNLI and NER), which could be considered a limitation.\\n\\nRegarding new scripts or languages being introduced, there is no explicit information in the provided context to discuss how these might affect the limitations of using mBERT for zero-shot cross-lingual tasks.'}, 'done_reason': 'stop', 'done': True, 'total_duration': 9328346334, 'load_duration': 54800667, 'prompt_eval_count': 1043, 'prompt_eval_duration': 5131785000, 'eval_count': 107, 'eval_duration': 4135847000}, delta=None, logprobs=None, additional_kwargs={})})}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: RAG with Hierarchical indexing\n",
    "### Contextual Compression\n",
    "### Adaptive Retrieval\n",
    "### Sohisticated Controllable Agent\n",
    "### Recursive Retrieval,\n",
    "### Context Enrichement using QA, Summary\n",
    "### DocumentSummaryIndex Retrieval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unstructured_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
